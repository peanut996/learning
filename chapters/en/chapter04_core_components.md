# Chapter 4: Core Technical Components of LLMs

## 4.1 Text Preprocessing and Tokenization
- Subword segmentation algorithms (BPE, WordPiece, SentencePiece)
- Vocabulary construction
- Special token handling
- Differences between English and Chinese tokenization

## 4.2 Embedding Layers
- Concepts and roles of word embeddings
- Token Embedding
- Position Embedding
- Choosing embedding dimensions

## 4.3 Attention Mechanisms
- Self-Attention computation process
- Scaled Dot-Product Attention
- Parallel computation in Multi-Head Attention
- Visualization and interpretation of attention weights