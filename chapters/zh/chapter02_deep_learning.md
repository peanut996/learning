# 第2章：深度学习与神经网络基础

### 2.1 深度学习基本概念

#### 2.1.1 神经网络的基本原理
神经网络是深度学习的基础，受生物神经系统启发而设计的计算模型。

**生物神经元与人工神经元：**
- **生物神经元**：由细胞体、树突、轴突组成，通过电化学信号传递信息
- **人工神经元（感知机）**：数学模型，接收多个输入，通过权重加权求和，经激活函数产生输出
- **数学表示**：`y = f(∑(wi × xi) + b)`，其中f是激活函数，wi是权重，xi是输入，b是偏置

**神经网络的层次结构：**
- **输入层（Input Layer）**：接收原始数据，每个节点对应一个特征
- **隐藏层（Hidden Layers）**：进行特征变换和抽象，可以有多层
- **输出层（Output Layer）**：产生最终预测结果

**激活函数的作用：**
- **引入非线性**：没有激活函数，多层网络等价于单层线性变换
- **常见激活函数**：
  - **Sigmoid**: `σ(x) = 1/(1+e^(-x))`，输出范围(0,1)
  - **Tanh**: `tanh(x) = (e^x - e^(-x))/(e^x + e^(-x))`，输出范围(-1,1)
  - **ReLU**: `ReLU(x) = max(0,x)`，简单高效，缓解梯度消失
  - **Leaky ReLU**: `LeakyReLU(x) = max(αx,x)`，α通常为0.01
  - **GELU**: `GELU(x) = x × Φ(x)`，在Transformer中广泛使用

#### 2.1.2 前向传播和反向传播
神经网络的训练过程包括前向传播和反向传播两个核心步骤。

**前向传播（Forward Propagation）：**
- **计算过程**：输入数据从输入层逐层向前传播到输出层
- **层间计算**：每一层的输出 = 激活函数(权重矩阵 × 上一层输出 + 偏置)
- **最终输出**：网络产生预测结果
- **数学表示**：
  ```
  a^(l) = f^(l)(W^(l) × a^(l-1) + b^(l))
  ```
  其中a^(l)是第l层的激活值，W^(l)是权重矩阵，b^(l)是偏置向量

**反向传播（Backpropagation）：**
- **核心思想**：通过链式法则计算损失函数对各参数的梯度
- **梯度计算**：从输出层开始，逐层向前计算梯度
- **参数更新**：根据梯度和学习率更新权重和偏置
- **链式法则**：
  ```
  ∂L/∂W^(l) = ∂L/∂a^(l) × ∂a^(l)/∂z^(l) × ∂z^(l)/∂W^(l)
  ```

**梯度消失和梯度爆炸问题：**
- **梯度消失**：深层网络中梯度逐层衰减，导致前层参数更新缓慢
- **梯度爆炸**：梯度在反向传播中指数级增长，导致参数更新过大
- **解决方案**：
  - 梯度裁剪（Gradient Clipping）
  - 残差连接（Residual Connections）
  - 层归一化（Layer Normalization）
  - 合适的权重初始化策略

#### 2.1.3 梯度下降优化算法
梯度下降是神经网络训练的核心优化方法。

**基本梯度下降（BGD - Batch Gradient Descent）：**
- **更新规则**：`θ = θ - α × ∇J(θ)`
- **特点**：使用全部训练数据计算梯度
- **优点**：收敛稳定，能找到全局最优解（凸函数情况）
- **缺点**：计算量大，内存需求高，收敛速度慢

**随机梯度下降（SGD - Stochastic Gradient Descent）：**
- **更新规则**：每次使用单个样本计算梯度
- **特点**：高频率参数更新，引入随机性
- **优点**：计算效率高，能逃离局部最优
- **缺点**：收敛不稳定，可能在最优解附近震荡

**小批量梯度下降（Mini-batch Gradient Descent）：**
- **更新规则**：使用小批量数据（通常32-512个样本）计算梯度
- **平衡点**：在计算效率和收敛稳定性之间取得平衡
- **实际应用**：现代深度学习的标准做法

**高级优化器：**
- **Momentum**：
  ```
  v = βv + (1-β)∇J(θ)
  θ = θ - αv
  ```
  引入动量项，加速收敛，减少震荡

- **Adam（Adaptive Moment Estimation）**：
  ```
  m = β₁m + (1-β₁)∇J(θ)
  v = β₂v + (1-β₂)(∇J(θ))²
  θ = θ - α × m̂/√(v̂ + ε)
  ```
  自适应学习率，结合动量和RMSprop的优点

- **AdamW**：Adam的改进版本，解耦权重衰减
- **RMSprop**：自适应学习率，适合处理非平稳目标

#### 2.1.4 损失函数和评估指标
损失函数定义了模型预测与真实标签之间的差距。

**回归任务损失函数：**
- **均方误差（MSE）**：
  ```
  MSE = (1/n) × ∑(yi - ŷi)²
  ```
  对异常值敏感，常用于回归问题

- **平均绝对误差（MAE）**：
  ```
  MAE = (1/n) × ∑|yi - ŷi|
  ```
  对异常值较不敏感

- **Huber损失**：结合MSE和MAE的优点，在小误差时类似MSE，大误差时类似MAE

**分类任务损失函数：**
- **交叉熵损失（Cross-Entropy）**：
  ```
  CE = -∑yi × log(ŷi)
  ```
  最常用的分类损失函数，适合概率输出

- **二元交叉熵（Binary Cross-Entropy）**：二分类问题的特殊情况
- **Focal Loss**：解决类别不平衡问题，关注难分类样本

**评估指标：**
- **分类任务**：
  - 准确率（Accuracy）：正确预测的比例
  - 精确率（Precision）：正预测中实际为正的比例
  - 召回率（Recall）：实际为正中被预测为正的比例
  - F1分数：精确率和召回率的调和平均
  - AUC-ROC：ROC曲线下面积

- **回归任务**：
  - 均方根误差（RMSE）：MSE的平方根
  - 平均绝对百分比误差（MAPE）
  - 决定系数（R²）

### 2.2 序列建模基础

#### 2.2.1 循环神经网络(RNN)的局限性
RNN是早期处理序列数据的主要方法，但存在明显局限性。

**RNN的基本原理：**
- **循环结构**：隐藏状态在时间步之间传递信息
- **数学表示**：
  ```
  ht = tanh(Wxh × xt + Whh × ht-1 + bh)
  yt = Why × ht + by
  ```
- **参数共享**：所有时间步使用相同的权重矩阵

**RNN的主要局限性：**
- **梯度消失问题**：长序列中远距离依赖难以学习
  - 梯度在时间维度上指数衰减
  - 导致网络无法记住长期信息

- **梯度爆炸问题**：梯度可能指数级增长
  - 通过梯度裁剪可以缓解
  - 但仍然影响训练稳定性

- **顺序计算限制**：
  - 无法并行化处理序列
  - 训练和推理速度受限
  - 难以处理非常长的序列

- **信息瓶颈**：
  - 所有历史信息都压缩在固定大小的隐藏状态中
  - 容易丢失重要的历史信息

#### 2.2.2 长短期记忆网络(LSTM)
LSTM通过门控机制解决RNN的梯度消失问题。

**LSTM的核心思想：**
- **细胞状态（Cell State）**：信息的长期记忆载体
- **门控机制**：控制信息的流入、保留和输出
- **解决梯度消失**：通过加法运算保持梯度流动

**LSTM的三个门：**
- **遗忘门（Forget Gate）**：
  ```
  ft = σ(Wf × [ht-1, xt] + bf)
  ```
  决定从细胞状态中丢弃什么信息

- **输入门（Input Gate）**：
  ```
  it = σ(Wi × [ht-1, xt] + bi)
  C̃t = tanh(WC × [ht-1, xt] + bC)
  ```
  决定在细胞状态中存储什么新信息

- **输出门（Output Gate）**：
  ```
  ot = σ(Wo × [ht-1, xt] + bo)
  ht = ot × tanh(Ct)
  ```
  决定输出细胞状态的哪些部分

**细胞状态更新：**
```
Ct = ft × Ct-1 + it × C̃t
```

**LSTM的优势：**
- **缓解梯度消失**：细胞状态的加法更新保持梯度流动
- **选择性记忆**：门控机制允许网络学习何时记住、遗忘和输出
- **处理长序列**：能够建模更长的依赖关系

**LSTM的变体：**
- **Peephole LSTM**：门可以观察细胞状态
- **耦合遗忘和输入门**：简化门控结构
- **双向LSTM**：同时利用前向和后向信息

#### 2.2.3 门控循环单元(GRU)
GRU是LSTM的简化版本，减少了参数数量同时保持相似性能。

**GRU的设计理念：**
- **简化结构**：将LSTM的三个门简化为两个门
- **合并细胞状态和隐藏状态**：减少参数数量
- **保持关键功能**：仍能有效处理长序列依赖

**GRU的两个门：**
- **重置门（Reset Gate）**：
  ```
  rt = σ(Wr × [ht-1, xt] + br)
  ```
  控制对前一时刻隐藏状态的使用程度

- **更新门（Update Gate）**：
  ```
  zt = σ(Wz × [ht-1, xt] + bz)
  ```
  控制前一时刻隐藏状态有多少被保留到当前时刻

**隐藏状态更新：**
```
h̃t = tanh(Wh × [rt × ht-1, xt] + bh)
ht = (1 - zt) × ht-1 + zt × h̃t
```

**GRU vs LSTM：**
- **参数数量**：GRU比LSTM少约25%的参数
- **计算效率**：GRU训练和推理速度更快
- **性能对比**：在多数任务上性能相近，具体任务可能有差异
- **选择建议**：
  - 数据量小：选择GRU（参数少，不易过拟合）
  - 数据量大：可以尝试LSTM（表达能力更强）
  - 计算资源受限：选择GRU

#### 2.2.4 序列到序列(Seq2Seq)模型
Seq2Seq模型是处理序列转换任务的经典架构。

**Seq2Seq的基本架构：**
- **编码器（Encoder）**：将输入序列编码为固定长度的向量表示
- **解码器（Decoder）**：基于编码向量生成输出序列
- **应用场景**：机器翻译、文本摘要、对话生成等

**编码器设计：**
- **循环编码器**：使用LSTM/GRU处理输入序列
- **最终状态**：编码器的最后隐藏状态作为上下文向量
- **双向编码器**：结合前向和后向信息，提供更丰富的表示

**解码器设计：**
- **初始化**：使用编码器的最终状态初始化解码器
- **自回归生成**：每步生成依赖于前面已生成的内容
- **教师强制（Teacher Forcing）**：训练时使用真实标签作为输入

**注意力机制的引入：**
传统Seq2Seq的问题：
- **信息瓶颈**：所有输入信息压缩在单一向量中
- **长序列性能下降**：编码向量难以保留所有重要信息

**注意力机制解决方案：**
- **动态上下文**：解码器每步都能访问编码器的所有隐藏状态
- **权重计算**：
  ```
  eij = a(si-1, hj)  # 对齐分数
  αij = softmax(eij)  # 注意力权重
  ci = Σ αij × hj     # 上下文向量
  ```
- **改进效果**：显著提升长序列翻译质量

**Seq2Seq的局限性：**
- **串行解码**：无法并行生成，推理速度慢
- **长度限制**：处理很长序列时仍然困难
- **对齐问题**：输入输出序列长度差异很大时效果下降

这些局限性最终催生了Transformer架构的出现，彻底改变了序列建模的范式。

