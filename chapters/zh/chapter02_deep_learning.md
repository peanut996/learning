# 第2章：深度学习与神经网络基础

### 2.1 深度学习基本概念

#### 2.1.1 神经网络的基本原理
神经网络是深度学习的基础，受生物神经系统启发而设计的计算模型。

**生物神经元与人工神经元：**
- **生物神经元**：由细胞体、树突、轴突组成，通过电化学信号传递信息
- **人工神经元（感知机）**：数学模型，接收多个输入，通过权重加权求和，经激活函数产生输出
- **数学表示**：`y = f(∑(wi × xi) + b)`，其中f是激活函数，wi是权重，xi是输入，b是偏置

**神经网络的层次结构：**
- **输入层（Input Layer）**：接收原始数据，每个节点对应一个特征
- **隐藏层（Hidden Layers）**：进行特征变换和抽象，可以有多层
- **输出层（Output Layer）**：产生最终预测结果

**激活函数的作用：**
- **引入非线性**：没有激活函数，多层网络等价于单层线性变换
- **常见激活函数**：
  - **Sigmoid**: `σ(x) = 1/(1+e^(-x))`，输出范围(0,1)
  - **Tanh**: `tanh(x) = (e^x - e^(-x))/(e^x + e^(-x))`，输出范围(-1,1)
  - **ReLU**: `ReLU(x) = max(0,x)`，简单高效，缓解梯度消失
  - **Leaky ReLU**: `LeakyReLU(x) = max(αx,x)`，α通常为0.01
  - **GELU**: `GELU(x) = x × Φ(x)`，在Transformer中广泛使用

#### 2.1.2 前向传播和反向传播
神经网络的训练过程包括前向传播和反向传播两个核心步骤。

**前向传播（Forward Propagation）：**
- **计算过程**：输入数据从输入层逐层向前传播到输出层
- **层间计算**：每一层的输出 = 激活函数(权重矩阵 × 上一层输出 + 偏置)
- **最终输出**：网络产生预测结果
- **数学表示**：
  ```
  a^(l) = f^(l)(W^(l) × a^(l-1) + b^(l))
  ```
  其中a^(l)是第l层的激活值，W^(l)是权重矩阵，b^(l)是偏置向量

**反向传播（Backpropagation）：**
- **核心思想**：通过链式法则计算损失函数对各参数的梯度
- **梯度计算**：从输出层开始，逐层向前计算梯度
- **参数更新**：根据梯度和学习率更新权重和偏置
- **链式法则**：
  ```
  ∂L/∂W^(l) = ∂L/∂a^(l) × ∂a^(l)/∂z^(l) × ∂z^(l)/∂W^(l)
  ```

**梯度消失和梯度爆炸问题：**
- **梯度消失**：深层网络中梯度逐层衰减，导致前层参数更新缓慢
- **梯度爆炸**：梯度在反向传播中指数级增长，导致参数更新过大
- **解决方案**：
  - 梯度裁剪（Gradient Clipping）
  - 残差连接（Residual Connections）
  - 层归一化（Layer Normalization）
  - 合适的权重初始化策略

#### 2.1.3 梯度下降优化算法
梯度下降是神经网络训练的核心优化方法。

**基本梯度下降（BGD - Batch Gradient Descent）：**
- **更新规则**：`θ = θ - α × ∇J(θ)`
- **特点**：使用全部训练数据计算梯度
- **优点**：收敛稳定，能找到全局最优解（凸函数情况）
- **缺点**：计算量大，内存需求高，收敛速度慢

**随机梯度下降（SGD - Stochastic Gradient Descent）：**
- **更新规则**：每次使用单个样本计算梯度
- **特点**：高频率参数更新，引入随机性
- **优点**：计算效率高，能逃离局部最优
- **缺点**：收敛不稳定，可能在最优解附近震荡

**小批量梯度下降（Mini-batch Gradient Descent）：**
- **更新规则**：使用小批量数据（通常32-512个样本）计算梯度
- **平衡点**：在计算效率和收敛稳定性之间取得平衡
- **实际应用**：现代深度学习的标准做法

**高级优化器：**
- **Momentum**：
  ```
  v = βv + (1-β)∇J(θ)
  θ = θ - αv
  ```
  引入动量项，加速收敛，减少震荡

- **Adam（Adaptive Moment Estimation）**：
  ```
  m = β₁m + (1-β₁)∇J(θ)
  v = β₂v + (1-β₂)(∇J(θ))²
  θ = θ - α × m̂/√(v̂ + ε)
  ```
  自适应学习率，结合动量和RMSprop的优点

- **AdamW**：Adam的改进版本，解耦权重衰减
- **RMSprop**：自适应学习率，适合处理非平稳目标

#### 2.1.4 损失函数和评估指标
损失函数定义了模型预测与真实标签之间的差距。

**回归任务损失函数：**
- **均方误差（MSE）**：
  ```
  MSE = (1/n) × ∑(yi - ŷi)²
  ```
  对异常值敏感，常用于回归问题

- **平均绝对误差（MAE）**：
  ```
  MAE = (1/n) × ∑|yi - ŷi|
  ```
  对异常值较不敏感

- **Huber损失**：结合MSE和MAE的优点，在小误差时类似MSE，大误差时类似MAE

**分类任务损失函数：**
- **交叉熵损失（Cross-Entropy）**：
  ```
  CE = -∑yi × log(ŷi)
  ```
  最常用的分类损失函数，适合概率输出

- **二元交叉熵（Binary Cross-Entropy）**：二分类问题的特殊情况
- **Focal Loss**：解决类别不平衡问题，关注难分类样本

**评估指标：**
- **分类任务**：
  - 准确率（Accuracy）：正确预测的比例
  - 精确率（Precision）：正预测中实际为正的比例
  - 召回率（Recall）：实际为正中被预测为正的比例
  - F1分数：精确率和召回率的调和平均
  - AUC-ROC：ROC曲线下面积

- **回归任务**：
  - 均方根误差（RMSE）：MSE的平方根
  - 平均绝对百分比误差（MAPE）
  - 决定系数（R²）

### 2.2 序列建模基础

#### 2.2.1 循环神经网络(RNN)的局限性
RNN是早期处理序列数据的主要方法，但存在明显局限性。

**RNN的基本原理：**
- **循环结构**：隐藏状态在时间步之间传递信息
- **数学表示**：
  ```
  ht = tanh(Wxh × xt + Whh × ht-1 + bh)
  yt = Why × ht + by
  ```
- **参数共享**：所有时间步使用相同的权重矩阵

**RNN的主要局限性：**
- **梯度消失问题**：长序列中远距离依赖难以学习
  - 梯度在时间维度上指数衰减
  - 导致网络无法记住长期信息

- **梯度爆炸问题**：梯度可能指数级增长
  - 通过梯度裁剪可以缓解
  - 但仍然影响训练稳定性

- **顺序计算限制**：
  - 无法并行化处理序列
  - 训练和推理速度受限
  - 难以处理非常长的序列

- **信息瓶颈**：
  - 所有历史信息都压缩在固定大小的隐藏状态中
  - 容易丢失重要的历史信息

#### 2.2.2 长短期记忆网络(LSTM)
LSTM通过门控机制解决RNN的梯度消失问题。

**LSTM的核心思想：**
- **细胞状态（Cell State）**：信息的长期记忆载体
- **门控机制**：控制信息的流入、保留和输出
- **解决梯度消失**：通过加法运算保持梯度流动

**LSTM的三个门：**
- **遗忘门（Forget Gate）**：
  ```
  ft = σ(Wf × [ht-1, xt] + bf)
  ```
  决定从细胞状态中丢弃什么信息

- **输入门（Input Gate）**：
  ```
  it = σ(Wi × [ht-1, xt] + bi)
  C̃t = tanh(WC × [ht-1, xt] + bC)
  ```
  决定在细胞状态中存储什么新信息

- **输出门（Output Gate）**：
  ```
  ot = σ(Wo × [ht-1, xt] + bo)
  ht = ot × tanh(Ct)
  ```
  决定输出细胞状态的哪些部分

**细胞状态更新：**
```
Ct = ft × Ct-1 + it × C̃t
```

**LSTM的优势：**
- **缓解梯度消失**：细胞状态的加法更新保持梯度流动
- **选择性记忆**：门控机制允许网络学习何时记住、遗忘和输出
- **处理长序列**：能够建模更长的依赖关系

**LSTM的变体：**
- **Peephole LSTM**：门可以观察细胞状态
- **耦合遗忘和输入门**：简化门控结构
- **双向LSTM**：同时利用前向和后向信息

#### 2.2.3 门控循环单元(GRU)
GRU是LSTM的简化版本，减少了参数数量同时保持相似性能。

**GRU的设计理念：**
- **简化结构**：将LSTM的三个门简化为两个门
- **合并细胞状态和隐藏状态**：减少参数数量
- **保持关键功能**：仍能有效处理长序列依赖

**GRU的两个门：**
- **重置门（Reset Gate）**：
  ```
  rt = σ(Wr × [ht-1, xt] + br)
  ```
  控制对前一时刻隐藏状态的使用程度

- **更新门（Update Gate）**：
  ```
  zt = σ(Wz × [ht-1, xt] + bz)
  ```
  控制前一时刻隐藏状态有多少被保留到当前时刻

**隐藏状态更新：**
```
h̃t = tanh(Wh × [rt × ht-1, xt] + bh)
ht = (1 - zt) × ht-1 + zt × h̃t
```

**GRU vs LSTM：**
- **参数数量**：GRU比LSTM少约25%的参数
- **计算效率**：GRU训练和推理速度更快
- **性能对比**：在多数任务上性能相近，具体任务可能有差异
- **选择建议**：
  - 数据量小：选择GRU（参数少，不易过拟合）
  - 数据量大：可以尝试LSTM（表达能力更强）
  - 计算资源受限：选择GRU

#### 2.2.4 序列到序列(Seq2Seq)模型
Seq2Seq模型是处理序列转换任务的经典架构。

**Seq2Seq的基本架构：**
- **编码器（Encoder）**：将输入序列编码为固定长度的向量表示
- **解码器（Decoder）**：基于编码向量生成输出序列
- **应用场景**：机器翻译、文本摘要、对话生成等

**编码器设计：**
- **循环编码器**：使用LSTM/GRU处理输入序列
- **最终状态**：编码器的最后隐藏状态作为上下文向量
- **双向编码器**：结合前向和后向信息，提供更丰富的表示

**解码器设计：**
- **初始化**：使用编码器的最终状态初始化解码器
- **自回归生成**：每步生成依赖于前面已生成的内容
- **教师强制（Teacher Forcing）**：训练时使用真实标签作为输入

**注意力机制的引入：**
传统Seq2Seq的问题：
- **信息瓶颈**：所有输入信息压缩在单一向量中
- **长序列性能下降**：编码向量难以保留所有重要信息

**注意力机制解决方案：**
- **动态上下文**：解码器每步都能访问编码器的所有隐藏状态
- **权重计算**：
  ```
  eij = a(si-1, hj)  # 对齐分数
  αij = softmax(eij)  # 注意力权重
  ci = Σ αij × hj     # 上下文向量
  ```
- **改进效果**：显著提升长序列翻译质量

**Seq2Seq的局限性：**
- **串行解码**：无法并行生成，推理速度慢
- **长度限制**：处理很长序列时仍然困难
- **对齐问题**：输入输出序列长度差异很大时效果下降

这些局限性最终催生了Transformer架构的出现，彻底改变了序列建模的范式。

## 3. Transformer架构详解

### 3.1 Transformer的设计原理

#### 3.1.1 Self-Attention机制的核心思想
Self-Attention是Transformer的核心创新，彻底改变了序列建模的方式。

**传统序列建模的问题：**
- **顺序依赖**：RNN/LSTM必须按顺序处理，无法并行化
- **长距离依赖衰减**：信息在长序列中逐渐丢失
- **计算瓶颈**：隐藏状态成为信息传递的瓶颈

**Self-Attention的解决方案：**
- **直接建模关系**：任意两个位置之间可以直接计算关联度
- **并行计算**：所有位置可以同时处理，大幅提升效率
- **动态权重**：根据内容动态分配注意力权重

**Self-Attention的数学定义：**
```
Attention(Q,K,V) = softmax(QK^T/√dk)V
```
其中：
- Q (Query)：查询矩阵，表示"我要关注什么"
- K (Key)：键矩阵，表示"我能提供什么信息"
- V (Value)：值矩阵，表示"我具体包含什么内容"
- dk：键向量的维度，用于缩放防止梯度消失

**计算过程详解：**
1. **线性变换**：输入X通过三个不同的权重矩阵得到Q、K、V
   ```
   Q = XWQ,  K = XWK,  V = XWV
   ```

2. **相似度计算**：计算每个query与所有key的相似度
   ```
   scores = QK^T/√dk
   ```

3. **归一化**：使用softmax将相似度转换为概率分布
   ```
   weights = softmax(scores)
   ```

4. **加权求和**：根据权重对value进行加权平均
   ```
   output = weights × V
   ```

**Self-Attention的优势：**
- **全局感受野**：每个位置都能直接访问序列中的任意位置
- **计算复杂度**：对于长度n的序列，复杂度为O(n²)，相比RNN的O(n)在短序列时更优
- **可解释性**：注意力权重提供了直观的解释性
- **位置无关性**：打破了位置的强制顺序约束

#### 3.1.2 并行化处理的优势
Transformer的并行化能力是其相对于RNN/LSTM的重大优势。

**RNN的串行计算限制：**
```
h1 = f(x1, h0)
h2 = f(x2, h1)  # 必须等待h1计算完成
h3 = f(x3, h2)  # 必须等待h2计算完成
...
```

**Transformer的并行计算：**
```
# 所有位置可以同时计算
output1, output2, ..., outputn = Attention(Q, K, V)
```

**并行化的具体优势：**
- **训练加速**：可以充分利用GPU的并行计算能力
- **内存效率**：避免了RNN中需要保存所有中间状态的问题
- **梯度流动**：所有位置的梯度可以直接反向传播，避免梯度消失
- **硬件友好**：矩阵运算更适合现代GPU架构

**计算效率对比：**
- **RNN训练时间**：O(n) × 序列长度（串行）
- **Transformer训练时间**：O(1)（并行，受内存限制）
- **实际加速比**：在现代GPU上可达到10-100倍的训练加速

#### 3.1.3 Encoder-Decoder架构
原始Transformer采用经典的Encoder-Decoder架构，为不同任务提供了灵活的框架。

**整体架构概览：**
```
Input → Encoder → Context Representation → Decoder → Output
```

**Encoder的作用：**
- **特征提取**：将输入序列编码为高维表示
- **上下文建模**：捕获输入序列内部的依赖关系
- **多层堆叠**：通过多层编码器逐步抽象特征

**Decoder的作用：**
- **序列生成**：基于编码器输出生成目标序列
- **自回归生成**：每步生成依赖前面已生成的内容
- **条件生成**：结合编码器信息进行条件生成

**Encoder-Decoder的信息流：**
1. **编码阶段**：Encoder处理完整输入序列
2. **交互阶段**：Decoder通过Cross-Attention访问Encoder输出
3. **解码阶段**：Decoder自回归生成输出序列

**适用场景：**
- **机器翻译**：源语言→目标语言
- **文本摘要**：长文本→摘要
- **问答系统**：问题+文档→答案
- **代码生成**：自然语言→代码

### 3.2 关键组件分析

#### 3.2.1 Multi-Head Attention
Multi-Head Attention是Self-Attention的增强版本，通过多个"头"并行学习不同类型的关系。

**单头注意力的局限性：**
- **表达能力有限**：单一的注意力机制可能只能捕获一种类型的关系
- **信息瓶颈**：所有信息都通过同一个注意力通道传递

**Multi-Head设计理念：**
- **并行多头**：使用h个不同的注意力头同时工作
- **分工合作**：每个头可以专注学习不同类型的依赖关系
- **信息融合**：将多个头的输出拼接后再进行线性变换

**数学表示：**
```
MultiHead(Q,K,V) = Concat(head1, head2, ..., headh)WO

其中：headi = Attention(QWiQ, KWiK, VWiV)
```

**具体计算步骤：**
1. **投影分割**：将Q、K、V分别投影到h个子空间
   ```
   Qi = QWiQ,  Ki = KWiK,  Vi = VWiV
   维度：d_model → d_model/h
   ```

2. **并行计算**：每个头独立计算注意力
   ```
   headi = Attention(Qi, Ki, Vi)
   ```

3. **拼接融合**：将所有头的输出拼接
   ```
   MultiHead = Concat(head1, ..., headh)
   ```

4. **输出投影**：通过线性层得到最终输出
   ```
   Output = MultiHead × WO
   ```

**不同头的分工示例：**
- **Head 1**：关注语法关系（主谓宾结构）
- **Head 2**：关注语义关系（同义词、反义词）
- **Head 3**：关注长距离依赖（代词指代）
- **Head 4**：关注局部模式（短语结构）

**超参数选择：**
- **头数量（h）**：通常为8或16，过多会导致参数冗余
- **头维度（dk）**：通常设为d_model/h，保持总参数量平衡
- **经验法则**：h × dk = d_model，确保参数效率

#### 3.2.2 Position Encoding(位置编码)
由于Self-Attention本身是位置无关的，需要额外机制来建模位置信息。

**位置信息的重要性：**
- **语序敏感**：自然语言的含义高度依赖词序
- **语法结构**：位置信息对理解语法关系至关重要
- **时序建模**：很多任务需要理解事件的时间顺序

**绝对位置编码（原始Transformer）：**
使用三角函数编码绝对位置：
```
PE(pos, 2i) = sin(pos/10000^(2i/d_model))
PE(pos, 2i+1) = cos(pos/10000^(2i/d_model))
```

**设计优势：**
- **固定模式**：不需要学习，减少参数量
- **外推能力**：可以处理训练时未见过的序列长度
- **相对位置**：不同位置间的编码有固定的线性关系

**学习位置编码：**
- **可学习参数**：位置编码作为可训练参数
- **适应性强**：可以学习任务特定的位置模式
- **局限性**：难以外推到更长的序列

**相对位置编码：**
现代变体更多使用相对位置编码：
- **相对关系**：关注位置i和位置j之间的相对距离
- **旋转位置编码（RoPE）**：GPT等模型使用的高效编码方式
- **ALiBi**：线性偏置的位置编码方法

**位置编码的注入方式：**
- **加法注入**：PE + Token Embedding（原始方法）
- **拼接注入**：Concat(Token Embedding, PE)
- **乘法注入**：Token Embedding ⊗ PE

#### 3.2.3 Feed-Forward Networks
每个Transformer层都包含一个前馈网络，负责非线性变换和特征提取。

**FFN的结构：**
```
FFN(x) = max(0, xW1 + b1)W2 + b2
```
即：线性层 → ReLU激活 → 线性层

**具体实现：**
1. **扩展维度**：d_model → d_ff（通常d_ff = 4 × d_model）
2. **激活函数**：ReLU或GELU引入非线性
3. **压缩维度**：d_ff → d_model

**FFN的作用：**
- **非线性变换**：引入复杂的非线性变换能力
- **特征提取**：学习位置特定的特征变换
- **信息处理**：对注意力机制的输出进行进一步处理
- **表达增强**：增加模型的表达能力和容量

**激活函数选择：**
- **ReLU**：原始Transformer使用，简单高效
- **GELU**：现代LLM常用，更平滑的激活函数
- **SwiGLU**：一些最新模型使用，性能更优

**维度选择原理：**
- **扩展比例**：d_ff通常是d_model的4倍
- **计算平衡**：在表达能力和计算成本间平衡
- **经验法则**：更大的d_ff可以提升模型容量，但增加计算成本

#### 3.2.4 Layer Normalization
Layer Normalization是Transformer稳定训练的关键组件。

**归一化的必要性：**
- **梯度稳定**：防止梯度爆炸或消失
- **训练加速**：加快收敛速度
- **数值稳定**：维持激活值在合理范围内

**Layer Norm vs Batch Norm：**
- **Batch Norm**：在batch维度上归一化，适合CV任务
- **Layer Norm**：在特征维度上归一化，适合NLP任务
- **优势**：不依赖batch size，在序列建模中更稳定

**Layer Norm计算：**
```
μ = (1/d) × Σxi
σ² = (1/d) × Σ(xi - μ)²
LN(x) = γ × (x - μ)/√(σ² + ε) + β
```

**参数说明：**
- **μ, σ²**：该层的均值和方差
- **γ, β**：可学习的缩放和偏移参数
- **ε**：防止除零的小常数（通常为1e-6）

**Pre-Norm vs Post-Norm：**
- **Post-Norm（原始）**：Sublayer → Add & Norm
- **Pre-Norm（现代）**：Norm → Sublayer → Add
- **Pre-Norm优势**：训练更稳定，特别在深层网络中

#### 3.2.5 Residual Connections
残差连接是深层Transformer训练的关键技术。

**残差连接的原理：**
```
output = SubLayer(input) + input
```

**解决的问题：**
- **梯度消失**：为梯度提供直接的反向传播路径
- **信息丢失**：确保输入信息不会完全丢失
- **训练稳定性**：使深层网络更容易训练

**在Transformer中的应用：**
1. **Multi-Head Attention**：
   ```
   x' = x + MultiHeadAttention(LayerNorm(x))
   ```

2. **Feed-Forward Network**：
   ```
   x'' = x' + FFN(LayerNorm(x'))
   ```

**深度网络的挑战：**
- **退化问题**：网络加深时性能反而下降
- **优化困难**：深层网络难以训练到较浅网络的性能
- **残差解决方案**：让网络学习残差而非直接映射

**设计考虑：**
- **维度匹配**：残差连接要求输入输出维度相同
- **初始化策略**：残差路径的权重初始化很重要
- **梯度流动**：确保梯度能够顺畅地流向早期层

### 3.3 Transformer的变体

#### 3.3.1 GPT系列(Decoder-only)
GPT采用只有Decoder的架构，专注于自回归语言建模。

**架构特点：**
- **单向注意力**：只能看到当前位置之前的内容
- **因果掩码**：使用掩码确保不会"看到未来"
- **自回归生成**：逐个生成下一个token

**掩码机制：**
```
Mask Matrix (下三角矩阵):
[1, 0, 0, 0]
[1, 1, 0, 0]
[1, 1, 1, 0]
[1, 1, 1, 1]
```

**GPT的演进：**
- **GPT-1**：1.17亿参数，证明了预训练+微调的有效性
- **GPT-2**：15亿参数，展现了强大的文本生成能力
- **GPT-3**：1750亿参数，涌现出少样本学习能力
- **GPT-4**：参数未公开，多模态能力和推理能力显著提升

**训练目标：**
- **下一词预测**：给定前文，预测下一个词的概率分布
- **数学表示**：maximize Σ log P(wi|w1, w2, ..., wi-1)

**应用优势：**
- **文本生成**：擅长生成连贯的长文本
- **对话系统**：自然的对话生成能力
- **创意写作**：小说、诗歌、剧本创作
- **代码生成**：理解需求并生成代码

#### 3.3.2 BERT系列(Encoder-only)
BERT采用只有Encoder的架构，专注于语言理解任务。

**架构特点：**
- **双向注意力**：可以同时看到左右两侧的上下文
- **无生成能力**：不能进行自回归生成
- **深度理解**：专注于文本的深度理解和表示

**预训练任务：**
1. **掩码语言模型（MLM）**：
   - 随机掩盖15%的词汇
   - 预测被掩盖的词汇
   - 学习双向上下文表示

2. **下一句预测（NSP）**：
   - 判断两个句子是否连续
   - 学习句子间的关系
   - （在后续变体中被证明不太重要）

**BERT变体：**
- **RoBERTa**：移除NSP，优化训练策略
- **ALBERT**：参数共享，减少模型大小
- **DeBERTa**：解耦注意力机制，提升性能
- **DistilBERT**：知识蒸馏的轻量版本

**应用场景：**
- **文本分类**：情感分析、主题分类
- **命名实体识别**：识别人名、地名、组织名
- **问答系统**：阅读理解和知识问答
- **相似度计算**：文本相似度和检索

#### 3.3.3 T5系列(Encoder-Decoder)
T5保持了完整的Encoder-Decoder架构，将所有NLP任务统一为文本到文本的转换。

**"Text-to-Text"理念：**
- **统一框架**：所有任务都转换为文本生成问题
- **任务前缀**：通过前缀指定具体任务类型
- **示例**：
  - 翻译："translate English to German: Hello" → "Hallo"
  - 摘要："summarize: [long text]" → "[summary]"
  - 分类："classify: [text]" → "positive/negative"

**架构优势：**
- **灵活性**：同一模型可以处理多种任务
- **迁移学习**：任务间可以相互促进学习
- **统一接口**：简化了模型的使用和部署

**T5变体：**
- **T5-small/base/large/3B/11B**：不同规模的模型
- **mT5**：多语言版本
- **UL2**：统一语言学习者框架
- **PaLM-2**：基于T5架构的大规模模型

**训练策略：**
- **去噪自编码**：破坏输入文本，训练模型恢复
- **多任务学习**：同时在多个任务上训练
- **前缀LM**：结合自编码和自回归的优势

**适用场景：**
- **多任务处理**：需要处理多种NLP任务的场景
- **少样本学习**：快速适应新任务
- **条件生成**：基于特定条件生成文本
- **跨任务迁移**：利用任务间的相关性

## 4. LLM的核心技术组件

### 4.1 文本预处理与标记化(Tokenization)

#### 4.1.1 标记化的重要性
标记化是LLM文本处理的第一步，将原始文本转换为模型能够理解的数字表示。

**传统分词的局限性：**
- **词汇表爆炸**：完整词汇分词会导致词汇表过大，特别是对于形态变化丰富的语言
- **未知词问题**：训练时未见过的词汇无法处理
- **低频词处理**：大量低频词占用词汇表空间但训练不充分
- **跨语言一致性**：不同语言的分词标准差异很大

**子词分割的优势：**
- **平衡表达力和效率**：在字符级和词级之间找到平衡点
- **处理未知词**：通过子词组合可以表示训练时未见过的词
- **缓解数据稀疏性**：减少低频词的数量
- **跨语言统一**：为多语言模型提供一致的表示方法

#### 4.1.2 主要子词分割算法

**Byte Pair Encoding (BPE):**
- **核心思想**：迭代合并最频繁出现的字符或字符序列对
- **算法流程**：
  1. 初始化：将所有词分解为字符序列
  2. 统计相邻字符对的频率
  3. 合并频率最高的字符对
  4. 重复步骤2-3，直到达到预设的词汇表大小
  5. 应用学到的合并规则处理新文本

**BPE实例：**
```
初始词汇: ["low", "lower", "newest", "widest"]
字符级: ["l o w", "l o w e r", "n e w e s t", "w i d e s t"]

迭代过程:
1. 最频繁对: "e s" → 合并为 "es"
2. 最频繁对: "es t" → 合并为 "est"
3. 最频繁对: "l o" → 合并为 "lo"
...

最终结果: ["lo w", "lo w er", "new est", "wid est"]
```

**WordPiece算法:**
- **核心改进**：基于语言模型概率而非简单频率进行合并
- **合并准则**：选择能最大化训练数据似然的字符对
- **计算公式**：
  ```
  score(x,y) = count(xy) / (count(x) × count(y))
  ```
- **优势**：更好地保持语言学意义的完整性
- **应用**：Google的BERT模型使用WordPiece分词

**SentencePiece算法:**
- **语言无关性**：不依赖于预分词，直接处理原始文本
- **统一处理**：将空格也作为特殊字符处理，实现真正的端到端分词
- **多语言支持**：特别适合中文、日文等没有显式空格分隔的语言
- **实现方式**：
  ```
  原文: "Hello world"
  SentencePiece: ["▁Hello", "▁wor", "ld"]
  (▁ 表示原始空格位置)
  ```

#### 4.1.3 词汇表构建策略

**词汇表大小选择：**
- **小词汇表（8K-16K）**：
  - 优势：模型参数少，训练快速
  - 劣势：序列长度增加，表达能力受限
- **中等词汇表（32K-64K）**：
  - 平衡点：在效率和表达力间取得平衡
  - 主流选择：大多数LLM采用的规模
- **大词汇表（128K+）**：
  - 优势：更好的表达能力，更短的序列
  - 劣势：嵌入层参数大，计算开销高

**多语言词汇表设计：**
- **语言平衡**：确保各语言都有足够的代表性
- **脚本覆盖**：涵盖不同的文字系统（拉丁、中文、阿拉伯等）
- **采样策略**：根据目标语言分布调整训练数据采样比例

#### 4.1.4 特殊标记的处理

**基础特殊标记：**
- **[CLS]**：分类标记，通常位于序列开头，用于分类任务
- **[SEP]**：分隔标记，用于分隔不同的文本段
- **[PAD]**：填充标记，用于将批次中的序列对齐到相同长度
- **[UNK]**：未知标记，表示词汇表外的词汇
- **[MASK]**：掩码标记，用于掩码语言模型训练

**生成任务特殊标记：**
- **[BOS]/[SOS]**：序列开始标记
- **[EOS]**：序列结束标记
- **[EOD]**：文档结束标记

**格式化标记：**
```
<|system|>: 系统提示信息
<|user|>: 用户输入
<|assistant|>: 助手回复
<|endoftext|>: 文本结束
```

#### 4.1.5 中英文分词的差异

**英文分词特点：**
- **天然分隔**：空格天然提供词边界信息
- **形态变化**：需要处理词形变化（run/running/ran）
- **复合词**：需要处理复合词分割问题
- **缩写处理**：处理缩写形式（don't → do not）

**中文分词挑战：**
- **无明显分隔**：没有空格等天然分隔符
- **歧义切分**：同一句话可能有多种合理的分词方式
  ```
  例：研究生命的起源
  切分1：研究 / 生命 / 的 / 起源
  切分2：研究生 / 命 / 的 / 起源
  ```
- **词汇边界模糊**：词和短语的界限不清晰
- **新词层出不穷**：网络新词、专业术语的快速涌现

**多语言统一方案：**
- **SentencePiece统一处理**：不依赖预分词，统一处理各种语言
- **字符级回退**：对于识别困难的部分，回退到字符级处理
- **上下文感知**：利用上下文信息消除分词歧义

### 4.2 嵌入层(Embedding Layers)

#### 4.2.1 词嵌入的基本概念

**嵌入的必要性：**
- **稀疏表示问题**：One-hot编码导致高维稀疏向量，计算效率低
- **语义缺失**：独热编码无法捕获词汇间的语义关系
- **维度爆炸**：词汇表大小直接决定向量维度，难以扩展

**嵌入的优势：**
- **密集表示**：将稀疏的词汇映射到密集的低维空间
- **语义建模**：相似词汇在嵌入空间中距离更近
- **参数效率**：大幅减少模型参数数量
- **迁移能力**：预训练的嵌入可以迁移到下游任务

**嵌入的数学表示：**
```
one-hot: [0, 0, 1, 0, ..., 0] ∈ ℝ^V (V为词汇表大小)
embedding: [0.2, -0.1, 0.8, ..., 0.3] ∈ ℝ^d (d为嵌入维度)

嵌入矩阵: E ∈ ℝ^(V×d)
嵌入查找: embedding = E[token_id]
```

#### 4.2.2 Token Embedding的实现

**嵌入层的设计：**
- **查找表机制**：本质上是一个大型查找表，每个token ID对应一个向量
- **参数共享**：输入和输出嵌入层可以共享参数减少总参数量
- **初始化策略**：
  ```
  # 常用初始化方法
  Xavier初始化: weight ~ N(0, 1/sqrt(d))
  He初始化: weight ~ N(0, 2/sqrt(d))
  均匀分布: weight ~ U(-sqrt(3/d), sqrt(3/d))
  ```

**嵌入层的训练：**
- **端到端学习**：嵌入权重作为模型参数一起训练
- **梯度更新**：只有被使用的token对应的嵌入向量会收到梯度更新
- **正则化技术**：
  - Dropout：随机置零部分嵌入维度
  - Weight Decay：L2正则化防止过拟合
  - Gradient Clipping：防止梯度爆炸

#### 4.2.3 Position Embedding的详细分析

**位置信息的重要性重申：**
- **顺序敏感性**：自然语言高度依赖词序
- **语法结构**：位置信息对解析语法关系至关重要
- **语义差异**：相同词汇在不同位置可能有不同含义

**绝对位置编码深入：**

**Sinusoidal Position Encoding（正弦位置编码）：**
```
PE(pos, 2i) = sin(pos / 10000^(2i/d_model))
PE(pos, 2i+1) = cos(pos / 10000^(2i/d_model))
```

**设计原理分析：**
- **频率递减**：不同维度使用不同频率，从高频到低频
- **唯一性保证**：每个位置都有唯一的编码表示
- **相对位置关系**：利用三角函数的性质，相对位置有固定的线性关系
- **外推能力**：可以处理比训练序列更长的输入

**学习位置编码对比：**
```
优势：
- 适应任务特定的位置模式
- 可以学习复杂的位置关系
- 在有限长度内性能通常更好

劣势：
- 无法外推到更长序列
- 需要额外的参数存储
- 训练时间更长
```

**现代位置编码方案：**

**Rotary Position Embedding (RoPE)：**
- **核心思想**：通过旋转矩阵在复数空间中编码位置信息
- **数学表示**：
  ```
  f_q(x_m, m) = (W_q x_m) ⊗ e^(i m θ)
  f_k(x_n, n) = (W_k x_n) ⊗ e^(i n θ)
  ```
- **优势**：自然地编码相对位置关系，外推性能好

**ALiBi (Attention with Linear Biases)：**
- **机制**：直接在注意力分数上添加线性偏置
- **计算方式**：
  ```
  attention_score = QK^T + bias_matrix
  bias_matrix[i,j] = -m × |i-j|
  ```
- **优点**：简单高效，优秀的长度外推能力

#### 4.2.4 嵌入维度的选择原则

**维度选择的权衡：**
- **表达能力 vs 计算效率**：更高维度提供更强表达能力但增加计算成本
- **过拟合风险**：维度过高可能导致过拟合，特别是在小数据集上
- **下游任务需求**：不同任务对表示能力的要求不同

**经验法则：**
- **小型模型（<100M参数）**：128-512维
- **中型模型（100M-1B参数）**：512-1024维
- **大型模型（>1B参数）**：1024-4096维或更高

**维度与模型其他组件的关系：**
```
通常设置：
d_model = d_embedding
d_ff = 4 × d_model (Feed-Forward隐藏层维度)
d_head = d_model / num_heads (每个注意力头的维度)
```

### 4.3 注意力机制详解(Attention Mechanisms)

#### 4.3.1 Self-Attention的完整计算流程

**步骤1：线性变换**
```python
# 输入: X ∈ ℝ^(seq_len × d_model)
Q = X @ W_Q  # Query矩阵
K = X @ W_K  # Key矩阵
V = X @ W_V  # Value矩阵

# 权重矩阵: W_Q, W_K, W_V ∈ ℝ^(d_model × d_model)
```

**步骤2：注意力分数计算**
```python
# 计算注意力分数
scores = Q @ K.T  # ∈ ℝ^(seq_len × seq_len)

# 缩放因子
scaled_scores = scores / sqrt(d_model)
```

**步骤3：掩码应用（如果需要）**
```python
# 因果掩码（用于GPT等decoder-only模型）
mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1)
masked_scores = scaled_scores.masked_fill(mask == 1, -inf)
```

**步骤4：Softmax归一化**
```python
attention_weights = softmax(masked_scores, dim=-1)
# 确保每行和为1，表示概率分布
```

**步骤5：加权聚合**
```python
output = attention_weights @ V  # ∈ ℝ^(seq_len × d_model)
```

#### 4.3.2 Scaled Dot-Product Attention的设计原理

**缩放因子的必要性：**
- **梯度稳定性**：防止softmax函数进入饱和区导致梯度消失
- **数值稳定性**：避免点积过大导致的数值溢出
- **理论分析**：
  ```
  假设q_i, k_j为独立同分布，均值0，方差1
  则q_i · k_j的方差为d_k
  缩放后方差变为1，保持数值稳定
  ```

**注意力分布的性质：**
- **稀疏性**：经过softmax后，注意力通常集中在少数相关位置
- **平滑性**：温度参数（这里是√d_k）控制分布的平滑程度
- **可解释性**：注意力权重提供了模型决策的直观解释

#### 4.3.3 Multi-Head Attention的并行实现

**并行计算的实现技巧：**
```python
# 传统实现（串行）
outputs = []
for head in range(num_heads):
    Q_h = Q @ W_Q[head]
    K_h = K @ W_K[head]
    V_h = V @ W_V[head]
    output_h = attention(Q_h, K_h, V_h)
    outputs.append(output_h)
concat_output = concat(outputs)

# 高效实现（并行）
# 将所有头的权重矩阵拼接
W_Q_all = concat([W_Q[0], W_Q[1], ..., W_Q[h-1]], dim=1)
Q_all = X @ W_Q_all  # 一次矩阵乘法得到所有头的Q

# 重塑为多头格式
Q_heads = Q_all.reshape(batch_size, seq_len, num_heads, d_head)
Q_heads = Q_heads.transpose(1, 2)  # (batch, num_heads, seq_len, d_head)
```

**内存效率优化：**
- **Flash Attention**：通过分块计算减少内存使用
- **Gradient Checkpointing**：用计算换内存，重新计算前向过程
- **混合精度训练**：使用FP16减少内存占用

#### 4.3.4 Attention权重的可视化和解释

**注意力热力图：**
- **横轴**：输入序列的token位置
- **纵轴**：输出序列的token位置（或相同序列用于self-attention）
- **颜色深度**：注意力权重的大小

**不同层注意力的模式：**
- **浅层**：主要关注局部语法关系（如相邻词汇）
- **中层**：捕获中等距离的语义关系
- **深层**：建模长距离依赖和高级语义关系

**注意力头的专业化：**
通过分析实际训练的模型发现：
- **语法头**：专注于主谓宾等语法关系
- **共指头**：处理代词指代和名词引用
- **位置头**：主要关注相对位置信息
- **语义头**：捕获语义相似性和主题相关性

**注意力解释的局限性：**
- **因果性问题**：高注意力权重不一定表示因果关系
- **多头聚合**：单个头的解释可能不完整
- **非线性变换**：后续的FFN层会进一步变换注意力输出
- **训练动态性**：注意力模式在训练过程中不断变化

**实用的注意力分析方法：**
```python
# 注意力权重统计
def analyze_attention_patterns(attention_weights):
    # attention_weights: (batch, num_heads, seq_len, seq_len)

    # 计算注意力的局部性（关注相邻token的程度）
    locality_score = compute_locality_bias(attention_weights)

    # 计算注意力的分散程度
    entropy = compute_attention_entropy(attention_weights)

    # 识别关键注意力连接
    important_connections = find_important_attention_links(attention_weights)

    return {
        'locality': locality_score,
        'entropy': entropy,
        'key_connections': important_connections
    }
```

## 5. LLM的训练过程

### 5.1 预训练(Pretraining)

#### 5.1.1 预训练的目标和意义

**预训练的核心目标：**
- **语言表示学习**：从大量无标注文本中学习通用的语言表示
- **知识获取**：将人类知识编码到模型参数中
- **模式识别**：学习语言的统计规律和结构模式
- **迁移能力培养**：为下游任务提供强大的初始化参数

