# 第3章：Transformer架构详解

## 3.1 Transformer的设计原理

### 3.1.1 Self-Attention机制的核心思想
Self-Attention是Transformer的核心创新，彻底改变了序列建模的方式。

**传统序列建模的问题：**
- **顺序依赖**：RNN/LSTM必须按顺序处理，无法并行化
- **长距离依赖衰减**：信息在长序列中逐渐丢失
- **计算瓶颈**：隐藏状态成为信息传递的瓶颈

**Self-Attention的解决方案：**
- **直接建模关系**：任意两个位置之间可以直接计算关联度
- **并行计算**：所有位置可以同时处理，大幅提升效率
- **动态权重**：根据内容动态分配注意力权重

**Self-Attention的数学定义：**
```
Attention(Q,K,V) = softmax(QK^T/√dk)V
```
其中：
- Q (Query)：查询矩阵，表示"我要关注什么"
- K (Key)：键矩阵，表示"我能提供什么信息"
- V (Value)：值矩阵，表示"我具体包含什么内容"
- dk：键向量的维度，用于缩放防止梯度消失

**计算过程详解：**
1. **线性变换**：输入X通过三个不同的权重矩阵得到Q、K、V
   ```
   Q = XWQ,  K = XWK,  V = XWV
   ```

2. **相似度计算**：计算每个query与所有key的相似度
   ```
   scores = QK^T/√dk
   ```

3. **归一化**：使用softmax将相似度转换为概率分布
   ```
   weights = softmax(scores)
   ```

4. **加权求和**：根据权重对value进行加权平均
   ```
   output = weights × V
   ```

**Self-Attention的优势：**
- **全局感受野**：每个位置都能直接访问序列中的任意位置
- **计算复杂度**：对于长度n的序列，复杂度为O(n²)，相比RNN的O(n)在短序列时更优
- **可解释性**：注意力权重提供了直观的解释性
- **位置无关性**：打破了位置的强制顺序约束

### 3.1.2 并行化处理的优势
Transformer的并行化能力是其相对于RNN/LSTM的重大优势。

**RNN的串行计算限制：**
```
h1 = f(x1, h0)
h2 = f(x2, h1)  # 必须等待h1计算完成
h3 = f(x3, h2)  # 必须等待h2计算完成
...
```

**Transformer的并行计算：**
```
# 所有位置可以同时计算
output1, output2, ..., outputn = Attention(Q, K, V)
```

**并行化的具体优势：**
- **训练加速**：可以充分利用GPU的并行计算能力
- **内存效率**：避免了RNN中需要保存所有中间状态的问题
- **梯度流动**：所有位置的梯度可以直接反向传播，避免梯度消失
- **硬件友好**：矩阵运算更适合现代GPU架构

**计算效率对比：**
- **RNN训练时间**：O(n) × 序列长度（串行）
- **Transformer训练时间**：O(1)（并行，受内存限制）
- **实际加速比**：在现代GPU上可达到10-100倍的训练加速

### 3.1.3 Encoder-Decoder架构
原始Transformer采用经典的Encoder-Decoder架构，为不同任务提供了灵活的框架。

**整体架构概览：**
```
Input → Encoder → Context Representation → Decoder → Output
```

**Encoder的作用：**
- **特征提取**：将输入序列编码为高维表示
- **上下文建模**：捕获输入序列内部的依赖关系
- **多层堆叠**：通过多层编码器逐步抽象特征

**Decoder的作用：**
- **序列生成**：基于编码器输出生成目标序列
- **自回归生成**：每步生成依赖前面已生成的内容
- **条件生成**：结合编码器信息进行条件生成

**Encoder-Decoder的信息流：**
1. **编码阶段**：Encoder处理完整输入序列
2. **交互阶段**：Decoder通过Cross-Attention访问Encoder输出
3. **解码阶段**：Decoder自回归生成输出序列

**适用场景：**
- **机器翻译**：源语言→目标语言
- **文本摘要**：长文本→摘要
- **问答系统**：问题+文档→答案
- **代码生成**：自然语言→代码

## 3.2 关键组件分析

### 3.2.1 Multi-Head Attention
Multi-Head Attention是Self-Attention的增强版本，通过多个"头"并行学习不同类型的关系。

**单头注意力的局限性：**
- **表达能力有限**：单一的注意力机制可能只能捕获一种类型的关系
- **信息瓶颈**：所有信息都通过同一个注意力通道传递

**Multi-Head设计理念：**
- **并行多头**：使用h个不同的注意力头同时工作
- **分工合作**：每个头可以专注学习不同类型的依赖关系
- **信息融合**：将多个头的输出拼接后再进行线性变换

**数学表示：**
```
MultiHead(Q,K,V) = Concat(head1, head2, ..., headh)WO

其中：headi = Attention(QWiQ, KWiK, VWiV)
```

**具体计算步骤：**
1. **投影分割**：将Q、K、V分别投影到h个子空间
   ```
   Qi = QWiQ,  Ki = KWiK,  Vi = VWiV
   维度：d_model → d_model/h
   ```

2. **并行计算**：每个头独立计算注意力
   ```
   headi = Attention(Qi, Ki, Vi)
   ```

3. **拼接融合**：将所有头的输出拼接
   ```
   MultiHead = Concat(head1, ..., headh)
   ```

4. **输出投影**：通过线性层得到最终输出
   ```
   Output = MultiHead × WO
   ```

**不同头的分工示例：**
- **Head 1**：关注语法关系（主谓宾结构）
- **Head 2**：关注语义关系（同义词、反义词）
- **Head 3**：关注长距离依赖（代词指代）
- **Head 4**：关注局部模式（短语结构）

**超参数选择：**
- **头数量（h）**：通常为8或16，过多会导致参数冗余
- **头维度（dk）**：通常设为d_model/h，保持总参数量平衡
- **经验法则**：h × dk = d_model，确保参数效率

### 3.2.2 Position Encoding(位置编码)
由于Self-Attention本身是位置无关的，需要额外机制来建模位置信息。

**位置信息的重要性：**
- **语序敏感**：自然语言的含义高度依赖词序
- **语法结构**：位置信息对理解语法关系至关重要
- **时序建模**：很多任务需要理解事件的时间顺序

**绝对位置编码（原始Transformer）：**
使用三角函数编码绝对位置：
```
PE(pos, 2i) = sin(pos/10000^(2i/d_model))
PE(pos, 2i+1) = cos(pos/10000^(2i/d_model))
```

**设计优势：**
- **固定模式**：不需要学习，减少参数量
- **外推能力**：可以处理训练时未见过的序列长度
- **相对位置**：不同位置间的编码有固定的线性关系

**学习位置编码：**
- **可学习参数**：位置编码作为可训练参数
- **适应性强**：可以学习任务特定的位置模式
- **局限性**：难以外推到更长的序列

**相对位置编码：**
现代变体更多使用相对位置编码：
- **相对关系**：关注位置i和位置j之间的相对距离
- **旋转位置编码（RoPE）**：GPT等模型使用的高效编码方式
- **ALiBi**：线性偏置的位置编码方法

**位置编码的注入方式：**
- **加法注入**：PE + Token Embedding（原始方法）
- **拼接注入**：Concat(Token Embedding, PE)
- **乘法注入**：Token Embedding ⊗ PE

### 3.2.3 Feed-Forward Networks
每个Transformer层都包含一个前馈网络，负责非线性变换和特征提取。

**FFN的结构：**
```
FFN(x) = max(0, xW1 + b1)W2 + b2
```
即：线性层 → ReLU激活 → 线性层

**具体实现：**
1. **扩展维度**：d_model → d_ff（通常d_ff = 4 × d_model）
2. **激活函数**：ReLU或GELU引入非线性
3. **压缩维度**：d_ff → d_model

**FFN的作用：**
- **非线性变换**：引入复杂的非线性变换能力
- **特征提取**：学习位置特定的特征变换
- **信息处理**：对注意力机制的输出进行进一步处理
- **表达增强**：增加模型的表达能力和容量

**激活函数选择：**
- **ReLU**：原始Transformer使用，简单高效
- **GELU**：现代LLM常用，更平滑的激活函数
- **SwiGLU**：一些最新模型使用，性能更优

**维度选择原理：**
- **扩展比例**：d_ff通常是d_model的4倍
- **计算平衡**：在表达能力和计算成本间平衡
- **经验法则**：更大的d_ff可以提升模型容量，但增加计算成本

### 3.2.4 Layer Normalization
Layer Normalization是Transformer稳定训练的关键组件。

**归一化的必要性：**
- **梯度稳定**：防止梯度爆炸或消失
- **训练加速**：加快收敛速度
- **数值稳定**：维持激活值在合理范围内

**Layer Norm vs Batch Norm：**
- **Batch Norm**：在batch维度上归一化，适合CV任务
- **Layer Norm**：在特征维度上归一化，适合NLP任务
- **优势**：不依赖batch size，在序列建模中更稳定

**Layer Norm计算：**
```
μ = (1/d) × Σxi
σ² = (1/d) × Σ(xi - μ)²
LN(x) = γ × (x - μ)/√(σ² + ε) + β
```

**参数说明：**
- **μ, σ²**：该层的均值和方差
- **γ, β**：可学习的缩放和偏移参数
- **ε**：防止除零的小常数（通常为1e-6）

**Pre-Norm vs Post-Norm：**
- **Post-Norm（原始）**：Sublayer → Add & Norm
- **Pre-Norm（现代）**：Norm → Sublayer → Add
- **Pre-Norm优势**：训练更稳定，特别在深层网络中

### 3.2.5 Residual Connections
残差连接是深层Transformer训练的关键技术。

**残差连接的原理：**
```
output = SubLayer(input) + input
```

**解决的问题：**
- **梯度消失**：为梯度提供直接的反向传播路径
- **信息丢失**：确保输入信息不会完全丢失
- **训练稳定性**：使深层网络更容易训练

**在Transformer中的应用：**
1. **Multi-Head Attention**：
   ```
   x' = x + MultiHeadAttention(LayerNorm(x))
   ```

2. **Feed-Forward Network**：
   ```
   x'' = x' + FFN(LayerNorm(x'))
   ```

**深度网络的挑战：**
- **退化问题**：网络加深时性能反而下降
- **优化困难**：深层网络难以训练到较浅网络的性能
- **残差解决方案**：让网络学习残差而非直接映射

**设计考虑：**
- **维度匹配**：残差连接要求输入输出维度相同
- **初始化策略**：残差路径的权重初始化很重要
- **梯度流动**：确保梯度能够顺畅地流向早期层

## 3.3 Transformer的变体

### 3.3.1 GPT系列(Decoder-only)
GPT采用只有Decoder的架构，专注于自回归语言建模。

**架构特点：**
- **单向注意力**：只能看到当前位置之前的内容
- **因果掩码**：使用掩码确保不会"看到未来"
- **自回归生成**：逐个生成下一个token

**掩码机制：**
```
Mask Matrix (下三角矩阵):
[1, 0, 0, 0]
[1, 1, 0, 0]
[1, 1, 1, 0]
[1, 1, 1, 1]
```

**GPT的演进：**
- **GPT-1**：1.17亿参数，证明了预训练+微调的有效性
- **GPT-2**：15亿参数，展现了强大的文本生成能力
- **GPT-3**：1750亿参数，涌现出少样本学习能力
- **GPT-4**：参数未公开，多模态能力和推理能力显著提升

**训练目标：**
- **下一词预测**：给定前文，预测下一个词的概率分布
- **数学表示**：maximize Σ log P(wi|w1, w2, ..., wi-1)

**应用优势：**
- **文本生成**：擅长生成连贯的长文本
- **对话系统**：自然的对话生成能力
- **创意写作**：小说、诗歌、剧本创作
- **代码生成**：理解需求并生成代码

### 3.3.2 BERT系列(Encoder-only)
BERT采用只有Encoder的架构，专注于语言理解任务。

**架构特点：**
- **双向注意力**：可以同时看到左右两侧的上下文
- **无生成能力**：不能进行自回归生成
- **深度理解**：专注于文本的深度理解和表示

**预训练任务：**
1. **掩码语言模型（MLM）**：
   - 随机掩盖15%的词汇
   - 预测被掩盖的词汇
   - 学习双向上下文表示

2. **下一句预测（NSP）**：
   - 判断两个句子是否连续
   - 学习句子间的关系
   - （在后续变体中被证明不太重要）

**BERT变体：**
- **RoBERTa**：移除NSP，优化训练策略
- **ALBERT**：参数共享，减少模型大小
- **DeBERTa**：解耦注意力机制，提升性能
- **DistilBERT**：知识蒸馏的轻量版本

**应用场景：**
- **文本分类**：情感分析、主题分类
- **命名实体识别**：识别人名、地名、组织名
- **问答系统**：阅读理解和知识问答
- **相似度计算**：文本相似度和检索

### 3.3.3 T5系列(Encoder-Decoder)
T5保持了完整的Encoder-Decoder架构，将所有NLP任务统一为文本到文本的转换。

**"Text-to-Text"理念：**
- **统一框架**：所有任务都转换为文本生成问题
- **任务前缀**：通过前缀指定具体任务类型
- **示例**：
  - 翻译："translate English to German: Hello" → "Hallo"
  - 摘要："summarize: [long text]" → "[summary]"
  - 分类："classify: [text]" → "positive/negative"

**架构优势：**
- **灵活性**：同一模型可以处理多种任务
- **迁移学习**：任务间可以相互促进学习
- **统一接口**：简化了模型的使用和部署

**T5变体：**
- **T5-small/base/large/3B/11B**：不同规模的模型
- **mT5**：多语言版本
- **UL2**：统一语言学习者框架
- **PaLM-2**：基于T5架构的大规模模型

**训练策略：**
- **去噪自编码**：破坏输入文本，训练模型恢复
- **多任务学习**：同时在多个任务上训练
- **前缀LM**：结合自编码和自回归的优势

**适用场景：**
- **多任务处理**：需要处理多种NLP任务的场景
- **少样本学习**：快速适应新任务
- **条件生成**：基于特定条件生成文本
- **跨任务迁移**：利用任务间的相关性