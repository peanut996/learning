# 第4章：LLM的核心技术组件

## 4.1 文本预处理与标记化(Tokenization)

### 4.1.1 标记化的重要性
标记化是LLM文本处理的第一步，将原始文本转换为模型能够理解的数字表示。

**传统分词的局限性：**
- **词汇表爆炸**：完整词汇分词会导致词汇表过大，特别是对于形态变化丰富的语言
- **未知词问题**：训练时未见过的词汇无法处理
- **低频词处理**：大量低频词占用词汇表空间但训练不充分
- **跨语言一致性**：不同语言的分词标准差异很大

**子词分割的优势：**
- **平衡表达力和效率**：在字符级和词级之间找到平衡点
- **处理未知词**：通过子词组合可以表示训练时未见过的词
- **缓解数据稀疏性**：减少低频词的数量
- **跨语言统一**：为多语言模型提供一致的表示方法

#### 4.1.2 主要子词分割算法

**Byte Pair Encoding (BPE):**
- **核心思想**：迭代合并最频繁出现的字符或字符序列对
- **算法流程**：
  1. 初始化：将所有词分解为字符序列
  2. 统计相邻字符对的频率
  3. 合并频率最高的字符对
  4. 重复步骤2-3，直到达到预设的词汇表大小
  5. 应用学到的合并规则处理新文本

**BPE实例：**
```
初始词汇: ["low", "lower", "newest", "widest"]
字符级: ["l o w", "l o w e r", "n e w e s t", "w i d e s t"]

迭代过程:
1. 最频繁对: "e s" → 合并为 "es"
2. 最频繁对: "es t" → 合并为 "est"
3. 最频繁对: "l o" → 合并为 "lo"
...

最终结果: ["lo w", "lo w er", "new est", "wid est"]
```

**WordPiece算法:**
- **核心改进**：基于语言模型概率而非简单频率进行合并
- **合并准则**：选择能最大化训练数据似然的字符对
- **计算公式**：
  ```
  score(x,y) = count(xy) / (count(x) × count(y))
  ```
- **优势**：更好地保持语言学意义的完整性
- **应用**：Google的BERT模型使用WordPiece分词

**SentencePiece算法:**
- **语言无关性**：不依赖于预分词，直接处理原始文本
- **统一处理**：将空格也作为特殊字符处理，实现真正的端到端分词
- **多语言支持**：特别适合中文、日文等没有显式空格分隔的语言
- **实现方式**：
  ```
  原文: "Hello world"
  SentencePiece: ["▁Hello", "▁wor", "ld"]
  (▁ 表示原始空格位置)
  ```

#### 4.1.3 词汇表构建策略

**词汇表大小选择：**
- **小词汇表（8K-16K）**：
  - 优势：模型参数少，训练快速
  - 劣势：序列长度增加，表达能力受限
- **中等词汇表（32K-64K）**：
  - 平衡点：在效率和表达力间取得平衡
  - 主流选择：大多数LLM采用的规模
- **大词汇表（128K+）**：
  - 优势：更好的表达能力，更短的序列
  - 劣势：嵌入层参数大，计算开销高

**多语言词汇表设计：**
- **语言平衡**：确保各语言都有足够的代表性
- **脚本覆盖**：涵盖不同的文字系统（拉丁、中文、阿拉伯等）
- **采样策略**：根据目标语言分布调整训练数据采样比例

#### 4.1.4 特殊标记的处理

**基础特殊标记：**
- **[CLS]**：分类标记，通常位于序列开头，用于分类任务
- **[SEP]**：分隔标记，用于分隔不同的文本段
- **[PAD]**：填充标记，用于将批次中的序列对齐到相同长度
- **[UNK]**：未知标记，表示词汇表外的词汇
- **[MASK]**：掩码标记，用于掩码语言模型训练

**生成任务特殊标记：**
- **[BOS]/[SOS]**：序列开始标记
- **[EOS]**：序列结束标记
- **[EOD]**：文档结束标记

**格式化标记：**
```
<|system|>: 系统提示信息
<|user|>: 用户输入
<|assistant|>: 助手回复
<|endoftext|>: 文本结束
```

#### 4.1.5 中英文分词的差异

**英文分词特点：**
- **天然分隔**：空格天然提供词边界信息
- **形态变化**：需要处理词形变化（run/running/ran）
- **复合词**：需要处理复合词分割问题
- **缩写处理**：处理缩写形式（don't → do not）

**中文分词挑战：**
- **无明显分隔**：没有空格等天然分隔符
- **歧义切分**：同一句话可能有多种合理的分词方式
  ```
  例：研究生命的起源
  切分1：研究 / 生命 / 的 / 起源
  切分2：研究生 / 命 / 的 / 起源
  ```
- **词汇边界模糊**：词和短语的界限不清晰
- **新词层出不穷**：网络新词、专业术语的快速涌现

**多语言统一方案：**
- **SentencePiece统一处理**：不依赖预分词，统一处理各种语言
- **字符级回退**：对于识别困难的部分，回退到字符级处理
- **上下文感知**：利用上下文信息消除分词歧义

## 4.2 嵌入层(Embedding Layers)

### 4.2.1 词嵌入的基本概念

**嵌入的必要性：**
- **稀疏表示问题**：One-hot编码导致高维稀疏向量，计算效率低
- **语义缺失**：独热编码无法捕获词汇间的语义关系
- **维度爆炸**：词汇表大小直接决定向量维度，难以扩展

**嵌入的优势：**
- **密集表示**：将稀疏的词汇映射到密集的低维空间
- **语义建模**：相似词汇在嵌入空间中距离更近
- **参数效率**：大幅减少模型参数数量
- **迁移能力**：预训练的嵌入可以迁移到下游任务

**嵌入的数学表示：**
```
one-hot: [0, 0, 1, 0, ..., 0] ∈ ℝ^V (V为词汇表大小)
embedding: [0.2, -0.1, 0.8, ..., 0.3] ∈ ℝ^d (d为嵌入维度)

嵌入矩阵: E ∈ ℝ^(V×d)
嵌入查找: embedding = E[token_id]
```

### 4.2.2 Token Embedding的实现

**嵌入层的设计：**
- **查找表机制**：本质上是一个大型查找表，每个token ID对应一个向量
- **参数共享**：输入和输出嵌入层可以共享参数减少总参数量
- **初始化策略**：
  ```
  # 常用初始化方法
  Xavier初始化: weight ~ N(0, 1/sqrt(d))
  He初始化: weight ~ N(0, 2/sqrt(d))
  均匀分布: weight ~ U(-sqrt(3/d), sqrt(3/d))
  ```

**嵌入层的训练：**
- **端到端学习**：嵌入权重作为模型参数一起训练
- **梯度更新**：只有被使用的token对应的嵌入向量会收到梯度更新
- **正则化技术**：
  - Dropout：随机置零部分嵌入维度
  - Weight Decay：L2正则化防止过拟合
  - Gradient Clipping：防止梯度爆炸

### 4.2.3 Position Embedding的详细分析

**位置信息的重要性重申：**
- **顺序敏感性**：自然语言高度依赖词序
- **语法结构**：位置信息对解析语法关系至关重要
- **语义差异**：相同词汇在不同位置可能有不同含义

**绝对位置编码深入：**

**Sinusoidal Position Encoding（正弦位置编码）：**
```
PE(pos, 2i) = sin(pos / 10000^(2i/d_model))
PE(pos, 2i+1) = cos(pos / 10000^(2i/d_model))
```

**设计原理分析：**
- **频率递减**：不同维度使用不同频率，从高频到低频
- **唯一性保证**：每个位置都有唯一的编码表示
- **相对位置关系**：利用三角函数的性质，相对位置有固定的线性关系
- **外推能力**：可以处理比训练序列更长的输入

**学习位置编码对比：**
```
优势：
- 适应任务特定的位置模式
- 可以学习复杂的位置关系
- 在有限长度内性能通常更好

劣势：
- 无法外推到更长序列
- 需要额外的参数存储
- 训练时间更长
```

**现代位置编码方案：**

**Rotary Position Embedding (RoPE)：**
- **核心思想**：通过旋转矩阵在复数空间中编码位置信息
- **数学表示**：
  ```
  f_q(x_m, m) = (W_q x_m) ⊗ e^(i m θ)
  f_k(x_n, n) = (W_k x_n) ⊗ e^(i n θ)
  ```
- **优势**：自然地编码相对位置关系，外推性能好

**ALiBi (Attention with Linear Biases)：**
- **机制**：直接在注意力分数上添加线性偏置
- **计算方式**：
  ```
  attention_score = QK^T + bias_matrix
  bias_matrix[i,j] = -m × |i-j|
  ```
- **优点**：简单高效，优秀的长度外推能力

### 4.2.4 嵌入维度的选择原则

**维度选择的权衡：**
- **表达能力 vs 计算效率**：更高维度提供更强表达能力但增加计算成本
- **过拟合风险**：维度过高可能导致过拟合，特别是在小数据集上
- **下游任务需求**：不同任务对表示能力的要求不同

**经验法则：**
- **小型模型（<100M参数）**：128-512维
- **中型模型（100M-1B参数）**：512-1024维
- **大型模型（>1B参数）**：1024-4096维或更高

**维度与模型其他组件的关系：**
```
通常设置：
d_model = d_embedding
d_ff = 4 × d_model (Feed-Forward隐藏层维度)
d_head = d_model / num_heads (每个注意力头的维度)
```

## 4.3 注意力机制详解(Attention Mechanisms)

### 4.3.1 Self-Attention的完整计算流程

**步骤1：线性变换**
```python
# 输入: X ∈ ℝ^(seq_len × d_model)
Q = X @ W_Q  # Query矩阵
K = X @ W_K  # Key矩阵
V = X @ W_V  # Value矩阵

# 权重矩阵: W_Q, W_K, W_V ∈ ℝ^(d_model × d_model)
```

**步骤2：注意力分数计算**
```python
# 计算注意力分数
scores = Q @ K.T  # ∈ ℝ^(seq_len × seq_len)

# 缩放因子
scaled_scores = scores / sqrt(d_model)
```

**步骤3：掩码应用（如果需要）**
```python
# 因果掩码（用于GPT等decoder-only模型）
mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1)
masked_scores = scaled_scores.masked_fill(mask == 1, -inf)
```

**步骤4：Softmax归一化**
```python
attention_weights = softmax(masked_scores, dim=-1)
# 确保每行和为1，表示概率分布
```

**步骤5：加权聚合**
```python
output = attention_weights @ V  # ∈ ℝ^(seq_len × d_model)
```

### 4.3.2 Scaled Dot-Product Attention的设计原理

**缩放因子的必要性：**
- **梯度稳定性**：防止softmax函数进入饱和区导致梯度消失
- **数值稳定性**：避免点积过大导致的数值溢出
- **理论分析**：
  ```
  假设q_i, k_j为独立同分布，均值0，方差1
  则q_i · k_j的方差为d_k
  缩放后方差变为1，保持数值稳定
  ```

**注意力分布的性质：**
- **稀疏性**：经过softmax后，注意力通常集中在少数相关位置
- **平滑性**：温度参数（这里是√d_k）控制分布的平滑程度
- **可解释性**：注意力权重提供了模型决策的直观解释

### 4.3.3 Multi-Head Attention的并行实现

**并行计算的实现技巧：**
```python
# 传统实现（串行）
outputs = []
for head in range(num_heads):
    Q_h = Q @ W_Q[head]
    K_h = K @ W_K[head]
    V_h = V @ W_V[head]
    output_h = attention(Q_h, K_h, V_h)
    outputs.append(output_h)
concat_output = concat(outputs)

# 高效实现（并行）
# 将所有头的权重矩阵拼接
W_Q_all = concat([W_Q[0], W_Q[1], ..., W_Q[h-1]], dim=1)
Q_all = X @ W_Q_all  # 一次矩阵乘法得到所有头的Q

# 重塑为多头格式
Q_heads = Q_all.reshape(batch_size, seq_len, num_heads, d_head)
Q_heads = Q_heads.transpose(1, 2)  # (batch, num_heads, seq_len, d_head)
```

**内存效率优化：**
- **Flash Attention**：通过分块计算减少内存使用
- **Gradient Checkpointing**：用计算换内存，重新计算前向过程
- **混合精度训练**：使用FP16减少内存占用

### 4.3.4 Attention权重的可视化和解释

**注意力热力图：**
- **横轴**：输入序列的token位置
- **纵轴**：输出序列的token位置（或相同序列用于self-attention）
- **颜色深度**：注意力权重的大小

**不同层注意力的模式：**
- **浅层**：主要关注局部语法关系（如相邻词汇）
- **中层**：捕获中等距离的语义关系
- **深层**：建模长距离依赖和高级语义关系

**注意力头的专业化：**
通过分析实际训练的模型发现：
- **语法头**：专注于主谓宾等语法关系
- **共指头**：处理代词指代和名词引用
- **位置头**：主要关注相对位置信息
- **语义头**：捕获语义相似性和主题相关性

**注意力解释的局限性：**
- **因果性问题**：高注意力权重不一定表示因果关系
- **多头聚合**：单个头的解释可能不完整
- **非线性变换**：后续的FFN层会进一步变换注意力输出
- **训练动态性**：注意力模式在训练过程中不断变化

**实用的注意力分析方法：**
```python
# 注意力权重统计
def analyze_attention_patterns(attention_weights):
    # attention_weights: (batch, num_heads, seq_len, seq_len)

    # 计算注意力的局部性（关注相邻token的程度）
    locality_score = compute_locality_bias(attention_weights)

    # 计算注意力的分散程度
    entropy = compute_attention_entropy(attention_weights)

    # 识别关键注意力连接
    important_connections = find_important_attention_links(attention_weights)

    return {
        'locality': locality_score,
        'entropy': entropy,
        'key_connections': important_connections
    }
```
