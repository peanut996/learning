# AI 代理的有效上下文工程

**发布时间：** 2025年9月29日

在应用人工智能领域,提示工程成为关注焦点数年后,一个新术语逐渐突显出来:**上下文工程**。使用语言模型进行构建已不再仅仅是为提示词找到正确的词汇和短语,而更多地转向回答一个更广泛的问题:"什么样的上下文配置最有可能产生我们模型所期望的行为?"

**上下文**是指从大型语言模型(LLM)采样时包含的令牌集。当前的**工程**问题是:如何针对 LLM 的固有约束优化这些令牌的效用,以持续达成期望的结果。有效地操控 LLM 通常需要**基于上下文思考**——换句话说:在任何给定时刻考虑 LLM 可用的整体状态,以及该状态可能产生哪些潜在行为。

在这篇文章中,我们将探索上下文工程这一新兴艺术,并为构建可操控、有效的代理提供一个精炼的思维模型。

## 上下文工程 vs. 提示工程

在 Anthropic,我们将上下文工程视为提示工程的自然演进。提示工程是指为了获得最佳结果而编写和组织 LLM 指令的方法(参见[我们的文档](https://docs.anthropic.com/en/docs/build-with-claude/prompt-engineering/overview)以获取概述和有用的提示工程策略)。**上下文工程**是指在 LLM 推理期间策划和维护最优令牌集(信息)的一套策略,包括提示之外可能进入的所有其他信息。

在使用 LLM 进行工程的早期,提示是 AI 工程工作的最大组成部分,因为除了日常聊天交互之外的大多数用例都需要针对单次分类或文本生成任务优化的提示。顾名思义,提示工程的主要焦点是如何编写有效的提示,特别是系统提示。然而,随着我们转向工程能力更强的代理——这些代理可以在多轮推理和更长的时间范围内运行——我们需要管理整个上下文状态的策略(系统指令、工具、[模型上下文协议](https://modelcontextprotocol.io/docs/getting-started/intro)(MCP)、外部数据、消息历史等)。

在循环中运行的代理会生成越来越多**可能**与下一轮推理相关的数据,这些信息必须被循环地精炼。上下文工程是从不断演变的可能信息宇宙中[策划](https://x.com/karpathy/status/1937902205765607626?lang=en)进入有限上下文窗口内容的艺术与科学。

*与编写提示的离散任务相比,上下文工程是迭代的,每次我们决定传递给模型什么内容时,都会进行策划阶段。*

## 为什么上下文工程对构建能力强大的代理很重要

尽管速度很快,能够管理越来越大的数据量,我们观察到 LLM 像人类一样,在某个点会失去焦点或经历困惑。关于"大海捞针"风格基准测试的研究发现了[上下文腐烂](https://research.trychroma.com/context-rot)的概念:随着上下文窗口中令牌数量的增加,模型从该上下文中准确回忆信息的能力会下降。

虽然一些模型表现出更温和的退化,但这一特征在所有模型中都会出现。因此,上下文必须被视为边际收益递减的有限资源。像人类一样,人类具有[有限的工作记忆容量](https://journals.sagepub.com/doi/abs/10.1177/0963721409359277),LLM 在解析大量上下文时也有"注意力预算"可供利用。引入的每个新令牌都会在一定程度上消耗这个预算,这增加了需要仔细策划 LLM 可用令牌的必要性。

这种注意力稀缺性源于 LLM 的架构约束。LLM 基于 [Transformer 架构](https://arxiv.org/abs/1706.03762),它使每个令牌能够[关注每个其他令牌](https://huggingface.co/blog/Esmail-AGumaan/attention-is-all-you-need),覆盖整个上下文。这导致对于 n 个令牌产生 n² 的成对关系。

随着上下文长度的增加,模型捕获这些成对关系的能力被拉薄,在上下文大小和注意力焦点之间产生自然张力。此外,模型从训练数据分布中发展其注意力模式,其中较短序列通常比较长序列更常见。这意味着模型对上下文范围的依赖性具有较少的经验和较少的专门参数。

像[位置编码插值](https://arxiv.org/pdf/2306.15595)这样的技术允许模型通过将它们适应原始训练的较小上下文来处理更长的序列,尽管在令牌位置理解上会有一些退化。这些因素创造了一个性能梯度而不是硬悬崖:模型在更长的上下文中仍然保持高度能力,但与在较短上下文上的表现相比,在信息检索和长程推理方面可能表现出精度降低。

这些现实意味着深思熟虑的上下文工程对于构建能力强大的代理至关重要。

## 有效上下文的剖析

鉴于 LLM 受限于有限的注意力预算,**好的**上下文工程意味着找到**最小可能**的高信号令牌集,以最大化某些期望结果的可能性。实施这一实践说起来容易做起来难,但在下一节中,我们概述了这一指导原则在上下文不同组件中的实际含义。

**系统提示**应该极其清晰,使用简单直接的语言,在**正确的高度**为代理呈现想法。正确的高度是两种常见失败模式之间的黄金地带。在一个极端,我们看到工程师在提示中硬编码复杂、脆弱的逻辑,以引发精确的代理行为。这种方法随着时间的推移会产生脆弱性并增加维护复杂性。在另一个极端,工程师有时提供模糊的高级指导,未能为 LLM 提供所需输出的具体信号,或错误地假设共享上下文。最佳高度取得平衡:足够具体以有效指导行为,同时又足够灵活,为模型提供强有力的启发式方法来指导行为。

*在光谱的一端,我们看到脆弱的 if-else 硬编码提示,而在另一端,我们看到过于笼统或错误假设共享上下文的提示。*

我们建议将提示组织成不同的部分(如 `<background_information>`、`<instructions>`、`## Tool guidance`、`## Output description` 等),并使用 XML 标记或 Markdown 标题等技术来描述这些部分,尽管随着模型变得更加强大,提示的确切格式可能变得不那么重要。

无论您如何决定构建系统提示,您都应该努力寻求完全概述预期行为的最小信息集。(请注意,最小并不一定意味着简短;您仍然需要预先为代理提供足够的信息以确保它遵守所需的行为。)最好从使用可用的最佳模型测试最小提示开始,看看它在您的任务上的表现如何,然后根据初始测试期间发现的失败模式添加清晰的指令和示例来提高性能。

**工具**允许代理与其环境操作并在工作时引入新的、额外的上下文。因为工具定义了代理与其信息/行动空间之间的契约,所以工具促进效率非常重要,既通过返回令牌高效的信息,也通过鼓励高效的代理行为。

在[为 AI 代理编写工具——使用 AI 代理](https://www.anthropic.com/engineering/writing-tools-for-agents)中,我们讨论了构建被 LLM 充分理解且功能重叠最小的工具。类似于设计良好的代码库的功能,工具应该是自包含的、对错误稳健的,并且在其预期用途方面极其清晰。输入参数同样应该是描述性的、明确的,并发挥模型的固有优势。

我们看到的最常见的失败模式之一是臃肿的工具集,它们覆盖了太多功能或导致关于使用哪个工具的模糊决策点。如果人类工程师不能明确地说在给定情况下应该使用哪个工具,就不能期望 AI 代理做得更好。正如我们稍后将讨论的那样,为代理策划最小可行工具集还可以在长时间交互中实现更可靠的上下文维护和修剪。

提供示例,也称为少样本提示,是我们继续强烈建议的众所周知的最佳实践。然而,团队通常会将大量边缘案例塞进提示中,试图阐明 LLM 应该遵循的每个可能规则。我们不推荐这样做。相反,我们建议努力策划一组多样化的典型示例,有效地描绘代理的预期行为。对于 LLM 来说,示例是价值千言的"图片"。

我们对上下文不同组件(系统提示、工具、示例、消息历史等)的总体指导是:深思熟虑,保持您的上下文信息丰富但紧凑。现在让我们深入探讨在运行时动态检索上下文。

## 上下文检索和代理搜索

在[构建有效的 AI 代理](https://www.anthropic.com/research/building-effective-agents)中,我们强调了基于 LLM 的工作流和代理之间的差异。自从我们撰写该文章以来,我们已经倾向于代理的[简单定义](https://simonwillison.net/2025/Sep/18/agents/):LLM 在循环中自主使用工具。

与我们的客户一起工作,我们看到该领域正在汇聚到这个简单的范式。随着底层模型变得更加强大,代理的自主水平可以扩展:更智能的模型允许代理独立导航细微差别的问题空间并从错误中恢复。

我们现在看到工程师如何思考为代理设计上下文的转变。今天,许多 AI 原生应用程序采用某种形式的基于嵌入的推理前时间检索,以浮现代理可以推理的重要上下文。随着该领域转向更具代理性的方法,我们越来越多地看到团队用"即时"上下文策略增强这些检索系统。

与预先处理所有相关数据不同,使用"即时"方法构建的代理维护轻量级标识符(文件路径、存储的查询、网络链接等),并使用这些引用在运行时使用工具动态加载数据到上下文中。Anthropic 的代理编码解决方案 [Claude Code](https://www.anthropic.com/claude-code) 使用这种方法在大型数据库上执行复杂的数据分析。模型可以编写目标查询、存储结果,并利用 Bash 命令(如 head 和 tail)分析大量数据,而无需将完整的数据对象加载到上下文中。这种方法反映了人类认知:我们通常不会记住整个信息语料库,而是引入外部组织和索引系统(如文件系统、收件箱和书签)来按需检索相关信息。

除了存储效率,这些引用的元数据还提供了一种有效精炼行为的机制,无论是明确提供还是直观的。对于在文件系统中操作的代理来说,在 `tests` 文件夹中名为 `test_utils.py` 的文件的存在暗示着与位于 `src/core_logic/` 中同名文件不同的目的。文件夹层次结构、命名约定和时间戳都提供了重要信号,帮助人类和代理理解如何以及何时利用信息。

让代理自主导航和检索数据还支持渐进式披露——换句话说,允许代理通过探索逐步发现相关上下文。每次交互都会产生通知下一个决策的上下文:文件大小暗示复杂性;命名约定暗示目的;时间戳可以是相关性的代理。代理可以逐层组装理解,仅在工作记忆中维护必要的内容,并利用笔记策略实现额外的持久性。这种自我管理的上下文窗口使代理专注于相关子集,而不是淹没在详尽但可能无关的信息中。

当然,有一个权衡:运行时探索比检索预先计算的数据慢。不仅如此,还需要有主见和深思熟虑的工程,以确保 LLM 拥有有效导航其信息景观的正确工具和启发式方法。如果没有适当的指导,代理可能会通过误用工具、追逐死胡同或未能识别关键信息而浪费上下文。

在某些设置中,最有效的代理可能采用混合策略,预先检索一些数据以提高速度,并根据其自主权追求进一步的自主探索。"正确"自主水平的决策边界取决于任务。Claude Code 是采用这种混合模型的代理:[CLAUDE.md](http://claude.md) 文件被天真地预先放入上下文中,而 glob 和 grep 等原语允许它导航其环境并及时检索文件,有效地绕过陈旧索引和复杂语法树的问题。

混合策略可能更适合具有较少动态内容的上下文,例如法律或金融工作。随着模型能力的提高,代理设计将趋向于让智能模型智能地行动,人工策划逐渐减少。鉴于该领域的快速进展,"做最简单有效的事情"可能仍然是我们对在 Claude 上构建代理的团队的最佳建议。

### 长期任务的上下文工程

长期任务要求代理在令牌计数超过 LLM 上下文窗口的行动序列中保持连贯性、上下文和目标导向的行为。对于跨越数十分钟到多小时连续工作的任务,如大型代码库迁移或综合研究项目,代理需要专门的技术来解决上下文窗口大小限制。

等待更大的上下文窗口可能看起来是一个明显的策略。但在可预见的未来,所有大小的上下文窗口都可能受到上下文污染和信息相关性问题的影响——至少在需要最强代理性能的情况下。为了使代理能够在延长的时间范围内有效工作,我们开发了一些直接解决这些上下文污染约束的技术:压缩、结构化笔记和多代理架构。

**压缩**

压缩是将接近上下文窗口限制的对话进行总结,并使用摘要重新启动新的上下文窗口的实践。压缩通常作为上下文工程中推动更好长期连贯性的第一个杠杆。在其核心,压缩以高保真方式提炼上下文窗口的内容,使代理能够以最小的性能退化继续。

例如,在 Claude Code 中,我们通过将消息历史传递给模型来总结和压缩最关键的细节来实现这一点。模型保留架构决策、未解决的错误和实现细节,同时丢弃冗余的工具输出或消息。然后,代理可以使用这个压缩的上下文加上最近访问的五个文件继续。用户获得连续性,无需担心上下文窗口限制。

压缩的艺术在于选择保留什么与丢弃什么,因为过度激进的压缩可能导致丢失微妙但关键的上下文,其重要性只有在后来才变得明显。对于实施压缩系统的工程师,我们建议在复杂的代理跟踪上仔细调整您的提示。首先最大化召回率,以确保您的压缩提示捕获跟踪中的每个相关信息片段,然后通过消除多余内容来迭代提高精度。

低悬的多余内容的一个示例是清除工具调用和结果——一旦在消息历史深处调用了工具,代理为什么还需要再次看到原始结果?最安全、最轻触的压缩形式之一是工具结果清除,最近作为 [Claude 开发者平台上的功能](https://www.anthropic.com/news/context-management)推出。

**结构化笔记**

结构化笔记或代理记忆是一种技术,代理定期将笔记持久化到上下文窗口之外的内存中。这些笔记稍后会被拉回到上下文窗口中。

这种策略以最小的开销提供持久记忆。就像 Claude Code 创建待办事项列表,或您的自定义代理维护 NOTES.md 文件一样,这种简单的模式允许代理跨复杂任务跟踪进度,维护否则会在数十次工具调用中丢失的关键上下文和依赖关系。

[Claude 玩宝可梦](https://www.twitch.tv/claudeplayspokemon)展示了记忆如何在非编码领域转变代理能力。代理在数千个游戏步骤中维护精确的计数——跟踪目标,如"在过去的 1,234 个步骤中,我一直在 1 号路训练我的宝可梦,皮卡丘已经获得了 8 级,目标是 10 级。"在没有任何关于记忆结构的提示的情况下,它开发探索区域的地图,记住它解锁了哪些关键成就,并维护战斗策略的战略笔记,帮助它学习哪些攻击对不同的对手效果最好。

在上下文重置后,代理读取自己的笔记并继续多小时的训练序列或地牢探索。这种跨总结步骤的连贯性使得仅将所有信息保留在 LLM 的上下文窗口中时不可能实现的长期策略成为可能。

作为我们 [Sonnet 4.5 发布](https://www.anthropic.com/effective-context-engineering-for-ai-agents)的一部分,我们在 Claude 开发者平台上发布了公开测试版的[记忆工具](http://anthropic.com/news/context-management),它通过基于文件的系统使在上下文窗口之外存储和查询信息变得更容易。这允许代理随时间构建知识库,跨会话维护项目状态,并引用以前的工作而无需将所有内容保留在上下文中。

**子代理架构**

子代理架构提供了另一种绕过上下文限制的方法。与其一个代理试图在整个项目中维护状态,不如让专门的子代理用干净的上下文窗口处理集中的任务。主代理使用高级计划进行协调,而子代理执行深入的技术工作或使用工具查找相关信息。每个子代理可能进行广泛探索,使用数万个令牌或更多,但只返回其工作的浓缩、提炼摘要(通常为 1,000-2,000 个令牌)。

这种方法实现了明确的关注点分离——详细的搜索上下文保留在子代理内部,而主代理专注于综合和分析结果。这种模式在[我们如何构建多代理研究系统](https://www.anthropic.com/engineering/multi-agent-research-system)中讨论,在复杂的研究任务上显示出相对于单代理系统的显著改进。

这些方法之间的选择取决于任务特征。例如:

- 压缩为需要大量来回的任务维护对话流;
- 笔记在具有明确里程碑的迭代开发中表现出色;
- 多代理架构处理并行探索带来回报的复杂研究和分析。

即使模型继续改进,在扩展交互中保持连贯性的挑战仍将是构建更有效代理的核心。

## 结论

上下文工程代表了我们如何使用 LLM 构建的根本转变。随着模型变得更加强大,挑战不仅仅是制作完美的提示——而是在每一步深思熟虑地策划什么信息进入模型有限的注意力预算。无论您是为长期任务实施压缩、设计令牌高效的工具,还是使代理能够及时探索其环境,指导原则保持不变:找到最大化您期望结果可能性的最小高信号令牌集。

随着模型的改进,我们概述的技术将继续发展。我们已经看到更智能的模型需要更少的规定性工程,允许代理以更多的自主权运行。但即使能力扩展,将上下文视为宝贵的有限资源仍将是构建可靠、有效代理的核心。

立即在 Claude 开发者平台上开始使用上下文工程,并通过我们的[记忆和上下文管理](https://github.com/anthropics/claude-cookbooks/blob/main/tool_use/memory_cookbook.ipynb) cookbook 访问有用的提示和最佳实践。

## 致谢

由 Anthropic 应用 AI 团队撰写:Prithvi Rajasekaran、Ethan Dixon、Carly Ryan 和 Jeremy Hadfield,团队成员 Rafi Ayub、Hannah Moran、Cal Rueb 和 Connor Jennings 做出了贡献。特别感谢 Molly Vorwerck、Stuart Ritchie 和 Maggie Vo 的支持。
