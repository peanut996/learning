# 打造高效 AI Agent：Context Engineering 实践指南

**发布时间：** 2025年9月29日

在 AI 应用领域，Prompt Engineering（提示工程）火了好几年之后，一个新概念开始受到关注：**Context Engineering（上下文工程）**。现在用大语言模型做开发，已经不只是给提示词找对词汇和句式这么简单了，而是要回答一个更核心的问题："给模型什么样的上下文，最容易让它做出我们想要的行为？"

**Context（上下文）**说白了就是你输入给大语言模型（LLM）的那些内容（token）。现在的**工程**问题就是：怎么在 LLM 的限制条件下，让这些内容发挥最大价值，稳定地得到我们想要的结果。想要用好 LLM，关键是要**基于上下文来思考**——也就是说：随时考虑 LLM 能看到什么信息，这些信息会让它产生什么行为。

这篇文章里，我们会聊聊 Context Engineering 这门新手艺，给大家提供一个清晰的思路，帮助你构建好用、靠谱的 AI Agents。

## Context Engineering vs. Prompt Engineering

在 Anthropic，我们把 Context Engineering 看作是 Prompt Engineering 的升级版。Prompt Engineering 就是怎么写好提示词，让 LLM 给出最好的结果（可以看看[我们的文档](https://docs.anthropic.com/en/docs/build-with-claude/prompt-engineering/overview)了解更多技巧）。而 **Context Engineering** 则更进一步，它关注的是在 LLM 运行过程中，怎么精心挑选和维护最合适的信息集，不只是提示词，还包括所有其他可能输入的内容。

早期用 LLM 做开发时，主要就是写提示词，因为大部分场景都是做文本分类或生成这种一次性任务。但现在不一样了，我们开始构建更强大的 AI Agents——这些 Agents 能进行多轮思考，能长时间运行——所以我们需要管理整个上下文状态，包括系统指令、工具、[模型上下文协议](https://modelcontextprotocol.io/docs/getting-started/intro)（MCP）、外部数据、聊天历史等等。

AI Agents 在循环运行时，会不断产生大量**可能**对下一步有用的数据，这些信息需要不断筛选和提炼。Context Engineering 就是要从这个不断膨胀的信息海洋中，[精心挑选](https://x.com/karpathy/status/1937902205765607626?lang=en)出最关键的内容，塞进有限的上下文窗口里。

![Context Engineering](/img/llm/context-engineering.webp)
*写提示词是一次性的工作，而 Context Engineering 是持续迭代的——每次给模型传内容时，都要精心挑选。*

## 为什么 Context Engineering 对构建强大的 AI Agents 很重要

虽然 LLM 速度快，能处理海量数据，但我们发现它和人一样，处理太多信息时会分心、会犯糊涂。研究人员做了很多"大海捞针"式的测试，发现了一个叫 [Context Rot（上下文腐烂）](https://research.trychroma.com/context-rot)的现象：上下文窗口里塞的内容越多，模型从中准确找信息的能力就会下降。

所有模型都有这个问题，只是程度不同。所以，上下文就像一个收益递减的有限资源。就像人的[工作记忆容量有限](https://journals.sagepub.com/doi/abs/10.1177/0963721409359277)一样，LLM 处理大量上下文时也有个"注意力预算"。每加一个 token，都会消耗一点这个预算，所以我们得仔细挑选给 LLM 看什么。

这种注意力稀缺性，来自 LLM 的底层架构限制。LLM 基于 [Transformer 架构](https://arxiv.org/abs/1706.03762)，它让每个 token 都能[关注其他所有 token](https://huggingface.co/blog/Esmail-AGumaan/attention-is-all-you-need)，覆盖整个上下文。这会产生 n² 级别的成对关系（n 是 token 数）。

随着上下文越来越长，模型处理这些成对关系的能力就会被稀释，在上下文大小和注意力集中度之间产生矛盾。而且，模型是从训练数据中学习注意力模式的，训练数据里短文本比长文本多得多。这意味着模型处理长上下文的经验更少，专门的参数也更少。

虽然有[位置编码插值](https://arxiv.org/pdf/2306.15595)这类技术，能让模型通过调整来处理更长的序列，但理解 token 位置的能力会有所下降。这些因素造成的结果是：模型性能是逐渐下降而不是突然崩溃——模型在长上下文中还是很能干的，但在信息检索和长程推理方面，精度会比短上下文时低一些。

正因为这些现实情况，深思熟虑的 Context Engineering 对构建强大的 AI Agents 至关重要。

## 有效 Context 的关键要素

既然 LLM 的注意力预算有限，**好的** Context Engineering 就是要找到**最精简**的高质量内容集，来最大化得到理想结果的可能性。说起来简单，做起来难，下面我们来看看这个原则在不同组件中怎么应用。

**System Prompt（系统提示）**要特别清晰，用简单直白的话，在**合适的抽象层次**给 AI Agent 提供指导。什么是合适的抽象层次？就是要避开两个常见的坑。一个极端是，工程师在提示里硬编码一堆复杂、脆弱的 if-else 逻辑，想精确控制 Agent 的每个行为。这样做会让系统很脆弱，维护成本也高。另一个极端是，提供的指导太模糊、太高层，没给 LLM 足够的具体信号，或者错误地以为模型能理解你的言外之意。最佳的抽象层次在中间：既要具体到能有效指导行为，又要足够灵活，让模型有发挥的空间。

![calibrating-system-prompt](/img/llm/calibrating-system-prompt.webp)
*一端是脆弱的 if-else 硬编码提示，另一端是太笼统或错误假设上下文的提示。*

我们建议把提示分成不同的区块（比如 `<background_information>`、`<instructions>`、`## Tool guidance`、`## Output description` 等），用 XML 标签或 Markdown 标题来标记。不过随着模型越来越强，提示的具体格式可能会变得没那么重要。

不管你怎么写 System Prompt，都要努力找到那个最精简的信息集，能完整描述期望的行为就好。（注意，精简不等于简短；你还是得给 Agent 足够的信息，确保它按你想要的方式行事。）最好的做法是：先用最强的模型测试最简单的提示，看看效果怎么样，然后根据测试中发现的问题，逐步添加清晰的指令和示例来提升性能。

**Tools（工具）**让 AI Agent 能和环境互动，在工作时引入新的、额外的上下文。因为工具定义了 Agent 和它的信息/行动空间之间的接口，所以工具的效率特别重要——既要返回精简高效的信息，也要引导 Agent 高效行动。

在[为 AI Agents 编写工具](https://www.anthropic.com/engineering/writing-tools-for-agents)这篇文章里，我们讨论了怎么构建让 LLM 能充分理解、功能又不重叠的工具。就像写代码一样，工具应该是自包含的、能处理错误的，在用途上也要特别明确。输入参数同样要描述清楚、定义明确，还要发挥模型的天然优势。

我们看到的最常见的问题之一，就是工具集太臃肿——功能覆盖太多，或者让人搞不清该用哪个工具。如果人类工程师都说不清在某种情况下该用哪个工具，就别指望 AI Agent 能做得更好。后面我们还会讲到，给 Agent 精心挑选最小可行工具集，还能让长时间交互中的上下文维护和裁剪更可靠。

**Few-shot Prompting（少样本提示）**，也就是提供示例，是我们一直强烈推荐的最佳实践。不过，团队经常会把一堆边缘案例都塞进提示里，试图把 LLM 应该遵循的每条规则都讲清楚。我们不建议这么做。相反，我们建议精心挑选一组多样化的典型示例，有效地展示 Agent 应该有的行为。对 LLM 来说，示例就像"一图胜千言"。

我们对上下文各个组件（System Prompt、Tools、示例、消息历史等）的总体建议是：仔细考虑，保持上下文信息丰富但紧凑。接下来我们聊聊运行时动态检索上下文的问题。

## Context 检索和 Agent 搜索

在[构建有效的 AI Agents](https://www.anthropic.com/research/building-effective-agents) 这篇文章里，我们强调了基于 LLM 的工作流和 AI Agents 之间的区别。从那之后，我们越来越倾向于 Agent 的[简单定义](https://simonwillison.net/2025/Sep/18/agents/)：**LLM 在循环中自主使用工具**。

和客户合作的过程中，我们发现业界正在向这个简单的范式靠拢。随着底层模型越来越强，Agent 的自主性也能不断提升：更聪明的模型能让 Agent 独立应对复杂的问题，从错误中恢复。

我们现在看到，工程师思考如何为 Agent 设计上下文的方式正在转变。现在，很多 AI 应用都会在推理前用基于嵌入的检索，把重要的上下文提前找出来给 Agent 用。但随着大家转向更自主的 Agent 方法，越来越多团队开始用"即时"上下文策略来增强这些检索系统。

和提前处理所有相关数据不同，用"即时"方法构建的 Agent 只维护轻量级的标识符（文件路径、查询、网络链接等），在运行时用工具动态加载数据到上下文中。Anthropic 的编码工具 [Claude Code](https://www.anthropic.com/claude-code) 就用这种方法在大型数据库上做复杂的数据分析。模型可以写查询、存储结果，用 Bash 命令（比如 head 和 tail）分析大量数据，而不用把完整的数据对象都加载到上下文里。这种方法其实很像人类的认知过程：我们通常不会记住所有信息，而是用外部的组织和索引系统（比如文件系统、收件箱、书签）来按需检索相关信息。

除了存储效率，这些引用的元数据本身也提供了有效优化行为的机制。对在文件系统里操作的 Agent 来说，`tests` 文件夹里的 `test_utils.py` 文件，显然和 `src/core_logic/` 里同名的文件用途不一样。文件夹层级、命名规则、时间戳都提供了重要信号，帮助人类和 Agent 理解如何以及何时使用这些信息。

让 Agent 自主导航和检索数据，还支持渐进式披露——换句话说，让 Agent 通过探索逐步发现相关上下文。每次交互都会产生为下一个决策提供信息的上下文：文件大小暗示复杂度；命名规则暗示用途；时间戳可能表示相关性。Agent 可以逐层构建理解，只在工作记忆中保留必要的内容，并用笔记策略实现额外的持久化。这种自我管理的上下文窗口，能让 Agent 专注于相关的子集，而不是被大量可能无关的信息淹没。

当然，这也有代价：运行时探索比检索预先计算的数据慢。而且，需要精心设计工程方案，确保 LLM 有正确的工具和策略来有效导航它的信息空间。如果没有适当的指导，Agent 可能会误用工具、钻进死胡同、或者错过关键信息，白白浪费上下文。

在某些场景下，最有效的 Agent 可能采用混合策略：预先检索一些数据来提速，同时根据需要进行自主探索。"正确"的自主程度取决于具体任务。Claude Code 就采用了这种混合模型：[CLAUDE.md](http://claude.md) 文件会直接预先放入上下文，而 glob 和 grep 这些基础工具则让它能够导航环境、即时检索文件，有效避开了陈旧索引和复杂语法树的问题。

混合策略可能更适合内容变化较少的场景，比如法律或金融工作。随着模型能力提升，Agent 设计会越来越倾向于让聪明的模型自主行动，人工干预越来越少。考虑到这个领域进展飞快，"做最简单有效的事"可能仍然是我们给在 Claude 上构建 Agent 的团队的最佳建议。

### 长期任务的 Context Engineering

长期任务需要 Agent 在 token 数超过 LLM 上下文窗口的情况下，保持连贯性、记住上下文、朝着目标前进。对于那些要花几十分钟到几小时的任务（比如大型代码库迁移或综合研究项目），Agent 需要一些专门的技术来应对上下文窗口大小的限制。

等更大的上下文窗口看起来是个显而易见的策略。但可预见的未来里，所有大小的上下文窗口都会受到上下文污染和信息相关性问题的影响——至少在需要 Agent 表现最佳的场景下是这样。为了让 Agent 能在更长时间内有效工作，我们开发了一些直接解决上下文污染问题的技术：Compression（压缩）、Structured Notes（结构化笔记）和 Multi-Agent 架构。

**Compression（压缩）**

压缩就是把快满的对话进行总结，用摘要来重启一个新的上下文窗口。压缩通常是上下文工程中改善长期连贯性的第一个手段。它的核心思想是：尽可能高保真地提炼上下文窗口的内容，让 Agent 能以最小的性能损失继续工作。

比如，在 Claude Code 中，我们把消息历史传给模型，让它总结和压缩最关键的细节。模型会保留架构决策、未解决的 bug、实现细节，同时丢弃冗余的工具输出或消息。然后，Agent 就可以用这个压缩过的上下文加上最近访问的五个文件继续工作。用户得到连续性，不用担心上下文窗口限制。

压缩的艺术在于选择保留什么、丢弃什么，因为压缩太激进可能会丢掉一些当时看起来不重要、但后来才发现很关键的上下文。对于要实现压缩系统的工程师，我们建议在复杂的 Agent 运行轨迹上仔细调优你的提示。先最大化召回率，确保你的压缩提示能捕获轨迹中的每个相关信息片段，然后通过消除冗余内容来逐步提高精度。

最简单的冗余内容就是清除工具调用和结果——一旦工具在消息历史深处被调用过了，Agent 为什么还需要再次看到原始结果？工具结果清除是最安全、影响最小的压缩形式之一，最近作为 [Claude 开发者平台的功能](https://www.anthropic.com/news/context-management)推出了。

**Structured Notes（结构化笔记）**

结构化笔记，或者叫 Agent Memory（记忆），是一种让 Agent 定期把笔记存到上下文窗口之外的内存里的技术。这些笔记稍后可以被拉回到上下文窗口中。

这种策略开销很小，却能提供持久记忆。就像 Claude Code 创建待办事项列表，或者你的自定义 Agent 维护 NOTES.md 文件一样，这个简单的模式能让 Agent 跨复杂任务跟踪进度，维护那些在几十次工具调用后会丢失的关键上下文和依赖关系。

[Claude 玩宝可梦](https://www.twitch.tv/claudeplayspokemon)展示了记忆如何在非编码领域改变 Agent 的能力。Agent 在数千个游戏步骤中维护精确的计数——跟踪目标，比如"在过去的 1,234 步中，我一直在 1 号路训练我的宝可梦，皮卡丘已经升到 8 级了，目标是 10 级。"在没有任何关于记忆结构的提示的情况下，它自己开发了探索区域的地图，记住它解锁了哪些关键成就，还维护战斗策略的笔记，帮它学习哪些攻击对不同对手效果最好。

上下文重置后，Agent 读取自己的笔记，继续多小时的训练序列或地牢探索。这种跨总结步骤的连贯性，让那些仅靠 LLM 上下文窗口不可能实现的长期策略成为可能。

作为我们 [Sonnet 4.5 发布](https://www.anthropic.com/effective-context-engineering-for-ai-agents)的一部分，我们在 Claude 开发者平台上推出了公测版的[记忆工具](http://anthropic.com/news/context-management)，它通过基于文件的系统让在上下文窗口之外存储和查询信息变得更容易。这让 Agent 能随时间构建知识库，跨会话维护项目状态，引用以前的工作，而不用把所有内容都保留在上下文中。

**Multi-Agent 架构**

Multi-Agent 架构提供了另一种绕过上下文限制的方法。与其让一个 Agent 在整个项目中维护状态，不如让专门的子 Agent 用干净的上下文窗口处理专注的任务。主 Agent 负责高层规划和协调，而子 Agent 执行深入的技术工作或用工具查找相关信息。每个子 Agent 可能进行广泛探索，用掉几万个 token 甚至更多，但只返回浓缩、提炼过的摘要（通常是 1,000-2,000 个 token）给主 Agent。

这种方法实现了清晰的关注点分离——详细的搜索上下文保留在子 Agent 内部，而主 Agent 专注于综合和分析结果。这种模式在[我们如何构建多 Agent 研究系统](https://www.anthropic.com/engineering/multi-agent-research-system)中有讨论，在复杂的研究任务上，相比单 Agent 系统显示出了显著的改进。

这些方法之间的选择取决于任务特点。比如：

- **Compression**适合需要大量来回交互的任务，能保持对话流；
- **Structured Notes**在有明确里程碑的迭代开发中表现出色；
- **Multi-Agent 架构**适合处理需要并行探索才能带来回报的复杂研究和分析。

即使模型持续改进，在长时间交互中保持连贯性的挑战，仍然是构建更有效 AI Agents 的核心问题。

## 结论

Context Engineering 代表了我们使用 LLM 构建应用方式的根本转变。随着模型越来越强大，挑战不再只是写个完美的提示——而是要在每一步深思熟虑地选择什么信息进入模型有限的注意力预算。不管你是为长期任务实现压缩、设计高效的工具，还是让 Agent 能即时探索环境，核心原则始终不变：**找到最精简的高质量内容集，来最大化得到理想结果的可能性**。

随着模型的改进，我们介绍的这些技术也会持续演进。我们已经看到更聪明的模型需要更少的死板规则，能让 Agent 以更高的自主性运行。但即使能力不断扩展，把上下文当作宝贵的有限资源，仍将是构建可靠、有效 AI Agents 的核心。

现在就在 Claude 开发者平台上开始尝试 Context Engineering 吧，通过我们的[记忆和上下文管理](https://github.com/anthropics/claude-cookbooks/blob/main/tool_use/memory_cookbook.ipynb) cookbook 学习有用的提示和最佳实践。

## 致谢

由 Anthropic 应用 AI 团队撰写:Prithvi Rajasekaran、Ethan Dixon、Carly Ryan 和 Jeremy Hadfield,团队成员 Rafi Ayub、Hannah Moran、Cal Rueb 和 Connor Jennings 做出了贡献。特别感谢 Molly Vorwerck、Stuart Ritchie 和 Maggie Vo 的支持。
